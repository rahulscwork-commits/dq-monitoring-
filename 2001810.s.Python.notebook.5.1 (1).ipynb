{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_distribution_drift(self, sys: str, tbl: str, tbl_conf: Dict[str, Any], cur_df: Optional[DataFrame], partition_date: str):\n",
    "        \"\"\"\n",
    "        Enhanced distribution drift check (driver-side summaries).\n",
    "        - Numeric primary metrics: Wasserstein (bucket-based) + quantile deltas (q10/q50/q90)\n",
    "          - PSI emitted as diagnostic if available (from compute_numeric_psi_and_buckets)\n",
    "        - Categorical primary metrics: Jensen-Shannon (top-K) + entropy delta + top-K mass & churn\n",
    "          - Chi2 emitted as diagnostic if available (from compute_categorical_csi_and_buckets)\n",
    "        - Falls back to historical psi_buckets / csi_buckets records when no reference partition exists.\n",
    "        Emits metrics conforming to metrics_history_schema via self._emit_metric(...)\n",
    "        \"\"\"\n",
    "        # Local small helpers (use self._... if already present)\n",
    "        def _counts_to_prob_vector(counts, eps=EPSILON):\n",
    "            total = float(sum(counts))\n",
    "            if total <= 0:\n",
    "                n = max(1, len(counts))\n",
    "                return [1.0 / n for _ in counts]\n",
    "            return [max(float(c) / total, eps) for c in counts]\n",
    "\n",
    "        def _wasserstein_from_buckets(edges, cur_counts, ref_counts):\n",
    "            try:\n",
    "                if not edges or not cur_counts or not ref_counts:\n",
    "                    return None\n",
    "                n_bins = len(edges) - 1\n",
    "                if len(cur_counts) != n_bins or len(ref_counts) != n_bins:\n",
    "                    self.logger.warning(\"Bucket length mismatch for wasserstein.\")\n",
    "                    return None\n",
    "                cur_prob = _counts_to_prob_vector(cur_counts)\n",
    "                ref_prob = _counts_to_prob_vector(ref_counts)\n",
    "                cdf_cur = [sum(cur_prob[:i+1]) for i in range(n_bins)]\n",
    "                cdf_ref = [sum(ref_prob[:i+1]) for i in range(n_bins)]\n",
    "                w = 0.0\n",
    "                for i in range(n_bins):\n",
    "                    width = float(edges[i+1]) - float(edges[i])\n",
    "                    diff = abs(cdf_cur[i] - cdf_ref[i])\n",
    "                    w += diff * width\n",
    "                return float(w)\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Wasserstein computation error.\")\n",
    "                return None\n",
    "\n",
    "        def _jensen_shannon_from_maps(cur_map, ref_map, top_k=None, eps=EPSILON):\n",
    "            try:\n",
    "                cur_map = cur_map or {}\n",
    "                ref_map = ref_map or {}\n",
    "                all_keys = set(cur_map.keys()) | set(ref_map.keys())\n",
    "                if top_k and len(all_keys) > top_k:\n",
    "                    combined = {k: cur_map.get(k, 0) + ref_map.get(k, 0) for k in all_keys}\n",
    "                    top_keys = set(sorted(combined.keys(), key=lambda k: combined[k], reverse=True)[:top_k])\n",
    "                    def reduce_map(m):\n",
    "                        out = {}\n",
    "                        other = 0\n",
    "                        for k,v in m.items():\n",
    "                            if k in top_keys:\n",
    "                                out[k] = v\n",
    "                            else:\n",
    "                                other += v\n",
    "                        if other > 0:\n",
    "                            out[\"__OTHER__\"] = other\n",
    "                        return out\n",
    "                    cur_map = reduce_map(cur_map)\n",
    "                    ref_map = reduce_map(ref_map)\n",
    "                    all_keys = set(cur_map.keys()) | set(ref_map.keys())\n",
    "                keys = sorted(all_keys)\n",
    "                cur_counts = [float(cur_map.get(k, 0)) for k in keys]\n",
    "                ref_counts = [float(ref_map.get(k, 0)) for k in keys]\n",
    "                cur_prob = _counts_to_prob_vector(cur_counts, eps)\n",
    "                ref_prob = _counts_to_prob_vector(ref_counts, eps)\n",
    "                m = [(p + q) / 2.0 for p, q in zip(cur_prob, ref_prob)]\n",
    "                def kl(p_vec, q_vec):\n",
    "                    s = 0.0\n",
    "                    for p_val, q_val in zip(p_vec, q_vec):\n",
    "                        if p_val <= 0:\n",
    "                            continue\n",
    "                        s += p_val * math.log(p_val / q_val)\n",
    "                    return s\n",
    "                js = 0.5 * (kl(cur_prob, m) + kl(ref_prob, m))\n",
    "                return float(js)\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Jensen-Shannon computation error.\")\n",
    "                return None\n",
    "\n",
    "        def _entropy_from_map(m, eps=EPSILON):\n",
    "            counts = [float(v) for v in (m or {}).values()]\n",
    "            total = sum(counts)\n",
    "            if total <= 0:\n",
    "                return 0.0\n",
    "            probs = [max(c / total, eps) for c in counts]\n",
    "            return float(-sum(p * math.log(p) for p in probs))\n",
    "\n",
    "        def _topk_churn_and_mass_change(cur_map, ref_map, k=10):\n",
    "            cur_map = cur_map or {}\n",
    "            ref_map = ref_map or {}\n",
    "            topk_cur = [k for k,_ in sorted(cur_map.items(), key=lambda kv: kv[1], reverse=True)[:k]]\n",
    "            topk_ref = [k for k,_ in sorted(ref_map.items(), key=lambda kv: kv[1], reverse=True)[:k]]\n",
    "            removed = len([x for x in topk_ref if x not in topk_cur])\n",
    "            added = len([x for x in topk_cur if x not in topk_ref])\n",
    "            def topk_share(m, keys):\n",
    "                total = float(sum(m.values()) or 0.0)\n",
    "                if total == 0.0:\n",
    "                    return 0.0\n",
    "                return sum(m.get(k, 0) for k in keys) / total\n",
    "            cur_share = topk_share(cur_map, topk_cur)\n",
    "            ref_share = topk_share(ref_map, topk_ref)\n",
    "            mass_delta = cur_share - ref_share\n",
    "            churn = (removed + added) / float(k)\n",
    "            return float(mass_delta), removed, added, float(churn)\n",
    "\n",
    "        # --------------------\n",
    "        # Begin function logic\n",
    "        # --------------------\n",
    "        full_path = tbl_conf.get(\"full_table_path\")\n",
    "        if not full_path:\n",
    "            self.logger.error(f\"Missing 'full_table_path' for {sys}.{tbl}. Cannot perform drift checks.\")\n",
    "            return\n",
    "\n",
    "        if cur_df is None or df_is_empty(cur_df):\n",
    "            self.logger.warning(f\"Current DataFrame empty for {sys}.{tbl}@{partition_date}. Skipping distribution drift.\")\n",
    "            # mark uncomputable for configured features\n",
    "            for c in (tbl_conf.get(\"numeric_drift_cols\") or []):\n",
    "                self._mark_uncomputable(\"psi\", sys, tbl, c, partition_date=partition_date)\n",
    "            for c in (tbl_conf.get(\"categorical_cols\") or []):\n",
    "                self._mark_uncomputable(\"csi\", sys, tbl, c, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        # find reference partition and load it if possible\n",
    "        ref_date = None\n",
    "        try:\n",
    "            ref_date = self._get_ref_partition_date(full_path)\n",
    "        except Exception:\n",
    "            self.logger.debug(\"No ref partition date available via _get_ref_partition_date.\")\n",
    "        ref_df = None\n",
    "        if ref_date:\n",
    "            ref_df = self._load_and_cache_df(sys, tbl, tbl_conf, ref_date)\n",
    "            if ref_df is not None and df_is_empty(ref_df):\n",
    "                self.logger.warning(f\"Reference partition '{ref_date}' for {sys}.{tbl} is empty; ignoring reference.\")\n",
    "                ref_df = None\n",
    "\n",
    "        # load metrics history DF once (your existing logic)\n",
    "        if not self.metrics_history_loaded:\n",
    "            mh_dataset_name = OUTPUT_DATASETS.get(\"metrics_history_df\")\n",
    "            if mh_dataset_name:\n",
    "                try:\n",
    "                    self.metrics_history_df_cache = spark.read.table(mh_dataset_name)\n",
    "                    self.metrics_history_df_cache.cache()\n",
    "                    # trigger caching safely\n",
    "                    try:\n",
    "                        self.metrics_history_df_cache.count()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    self.metrics_history_loaded = True\n",
    "                except Exception:\n",
    "                    self.logger.exception(f\"Failed to load metrics history from '{mh_dataset_name}'.\")\n",
    "                    self.metrics_history_df_cache = None\n",
    "                    self.metrics_history_loaded = True\n",
    "            else:\n",
    "                self.logger.debug(\"No metrics_history_df configured.\")\n",
    "                self.metrics_history_loaded = True\n",
    "\n",
    "        mh_df = self.metrics_history_df_cache\n",
    "\n",
    "        # --------------------\n",
    "        # Numeric drift\n",
    "        # --------------------\n",
    "        numeric_drift_cols = tbl_conf.get(\"numeric_drift_cols\") or []\n",
    "        for col_config in numeric_drift_cols:\n",
    "            col_name = col_config if isinstance(col_config, str) else col_config.get(\"name\")\n",
    "            if not col_name:\n",
    "                continue\n",
    "            resolved_col = resolve_col_name(cur_df, col_name)\n",
    "            if not resolved_col:\n",
    "                self.logger.warning(f\"Numeric column '{col_name}' not found in {sys}.{tbl}.\")\n",
    "                self._mark_uncomputable(\"psi\", sys, tbl, col_name, partition_date=partition_date)\n",
    "                continue\n",
    "\n",
    "            # Attempt primary computation using your helper (produces psi & buckets), else fallback\n",
    "            psi_val = None\n",
    "            psi_edges = None\n",
    "            cur_counts = None\n",
    "            ref_counts = None\n",
    "            # compute using existing helper if available\n",
    "            try:\n",
    "                if \"compute_numeric_psi_and_buckets\" in globals():\n",
    "                    psi_val, psi_edges, cur_counts, ref_counts = compute_numeric_psi_and_buckets(\n",
    "                        resolved_col, cur_df, ref_df,\n",
    "                        bins=DRIFT_BINS,\n",
    "                        sample_size=int(self.static_thr.get(\"drift_sample_size\", DRIFT_SAMPLE_SIZE))\n",
    "                    )\n",
    "                else:\n",
    "                    # fallback: try to produce equal-width buckets from ref/current min/max\n",
    "                    psi_val, psi_edges, cur_counts, ref_counts = None, None, None, None\n",
    "            except Exception:\n",
    "                self.logger.exception(f\"Error computing numeric buckets for {resolved_col}.\")\n",
    "\n",
    "            # emit bucket snapshot if present (current counts)\n",
    "            if psi_edges is not None and cur_counts is not None:\n",
    "                metric_value_buckets = json.dumps({\"bucket_edges\": psi_edges, \"bucket_counts\": cur_counts}, default=str, ensure_ascii=False)\n",
    "                self._emit_metric({\n",
    "                    \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                    \"metric_type\": \"psi_buckets\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                    \"metric_value\": metric_value_buckets, \"metric_value_num\": None, \"threshold\": None, \"reference_value\": None,\n",
    "                    \"status\": \"PASS\", \"alert_flag\": None, \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                }, partition_date=partition_date)\n",
    "            else:\n",
    "                self._mark_uncomputable(\"psi_buckets\", sys, tbl, resolved_col, partition_date=partition_date)\n",
    "\n",
    "            # If no reference available, try to recover buckets & ref counts from historical psi_buckets\n",
    "            if psi_val is None and ref_df is None and mh_df is not None:\n",
    "                try:\n",
    "                    hist = mh_df.filter(\n",
    "                        (F.col(\"table_name\") == tbl) &\n",
    "                        (F.col(\"metric_type\") == \"psi_buckets\") &\n",
    "                        (F.col(\"column_name\") == resolved_col)\n",
    "                    ).orderBy(F.col(\"partition_date\").desc()).limit(MAX_LOOKBACK_SAMPLES).collect()\n",
    "                    for r in hist:\n",
    "                        parsed = _parse_buckets_from_metric_value(r[\"metric_value\"])\n",
    "                        if not parsed:\n",
    "                            continue\n",
    "                        fb_edges = parsed.get(\"bucket_edges\") or parsed.get(\"edges\") or parsed.get(\"edges\")\n",
    "                        fb_ref_counts = parsed.get(\"bucket_counts\") or parsed.get(\"counts\")\n",
    "                        if not fb_edges or not fb_ref_counts or len(fb_edges) - 1 != len(fb_ref_counts):\n",
    "                            self.logger.warning(f\"Skipping historical PSI record due to invalid bucket data: {r['partition_date']}\")\n",
    "                            continue\n",
    "                        try:\n",
    "                            bucketizer = Bucketizer(splits=[float(x) for x in fb_edges], inputCol=resolved_col, outputCol=\"_dq_bucket\", handleInvalid=\"skip\")\n",
    "                            cur_proc_for_fb = cur_df.select(F.regexp_replace(F.col(resolved_col), \",\", \"\").cast(DoubleType()).alias(resolved_col)).where(F.col(resolved_col).isNotNull())\n",
    "                            cur_b = bucketizer.transform(cur_proc_for_fb).groupBy(\"_dq_bucket\").count().withColumnRenamed(\"count\", \"cur_cnt\")\n",
    "                            cur_rows_fb = cur_b.orderBy(\"_dq_bucket\").collect()\n",
    "                            current_counts_fb = [0] * (len(fb_edges) - 1)\n",
    "                            for rr in cur_rows_fb:\n",
    "                                idx = int(rr[\"_dq_bucket\"])\n",
    "                                if 0 <= idx < len(current_counts_fb):\n",
    "                                    current_counts_fb[idx] = int(rr[\"cur_cnt\"])\n",
    "                            # compute psi using helper if present, else local\n",
    "                            if \"_compute_psi_from_counts\" in dir(self):\n",
    "                                psi_fb = self._compute_psi_from_counts(current_counts_fb, [int(c) for c in fb_ref_counts], EPSILON)\n",
    "                            else:\n",
    "                                psi_fb = _compute_psi_from_counts(current_counts_fb, [int(c) for c in fb_ref_counts], EPSILON)\n",
    "                            if psi_fb is not None:\n",
    "                                psi_val = psi_fb\n",
    "                                metric_value_cur_fb = json.dumps({\"bucket_edges\": fb_edges, \"bucket_counts\": current_counts_fb}, default=str, ensure_ascii=False)\n",
    "                                self._emit_metric({\n",
    "                                    \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                                    \"metric_type\": \"psi_buckets\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                                    \"metric_value\": metric_value_cur_fb, \"metric_value_num\": None, \"threshold\": None, \"reference_value\": None,\n",
    "                                    \"status\": \"PASS\", \"alert_flag\": None, \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                                }, partition_date=partition_date)\n",
    "                                self.logger.info(f\"Computed PSI using historical bins from {r['partition_date']}.\")\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            self.logger.exception(\"Error during PSI fallback computation with historical bins.\")\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    self.logger.exception(\"Error loading historical metrics for PSI fallback.\")\n",
    "\n",
    "            # Now compute primary numeric drift metrics: Wasserstein + quantile deltas\n",
    "            numeric_metrics = {}\n",
    "            try:\n",
    "                # If we have edges and ref_counts, compute wasserstein\n",
    "                if psi_edges is not None and cur_counts is not None and ref_counts is not None:\n",
    "                    numeric_metrics[\"wasserstein\"] = _wasserstein_from_buckets(psi_edges, cur_counts, ref_counts)\n",
    "                # also compute approximate quantiles (10/50/90)\n",
    "                try:\n",
    "                    probs = [0.1, 0.5, 0.9]\n",
    "                    if not df_is_empty(cur_df):\n",
    "                        cur_q = cur_df.approxQuantile(resolved_col, probs, 0.01)\n",
    "                    else:\n",
    "                        cur_q = []\n",
    "                    if ref_df is not None and not df_is_empty(ref_df):\n",
    "                        ref_q = ref_df.approxQuantile(resolved_col, probs, 0.01)\n",
    "                    else:\n",
    "                        ref_q = []\n",
    "                    if cur_q and ref_q and len(cur_q) == len(ref_q):\n",
    "                        numeric_metrics[\"q10_cur\"], numeric_metrics[\"q50_cur\"], numeric_metrics[\"q90_cur\"] = cur_q[0], cur_q[1], cur_q[2]\n",
    "                        numeric_metrics[\"q10_ref\"], numeric_metrics[\"q50_ref\"], numeric_metrics[\"q90_ref\"] = ref_q[0], ref_q[1], ref_q[2]\n",
    "                        numeric_metrics[\"median_shift_abs\"] = float(cur_q[1] - ref_q[1])\n",
    "                        numeric_metrics[\"median_shift_rel\"] = (float(cur_q[1] - ref_q[1]) / float(abs(ref_q[1]))) if ref_q[1] != 0 else None\n",
    "                except Exception:\n",
    "                    self.logger.exception(\"Quantile diagnostics failed for numeric column.\")\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Error assembling numeric metrics.\")\n",
    "\n",
    "            # Decide drift using thresholds from static config (uses W_THRESH ~ numeric_psi_max as placeholder)\n",
    "            try:\n",
    "                # decide: primary threshold use numeric_psi_max as placeholder for wasserstein if not tuned\n",
    "                w_val = numeric_metrics.get(\"wasserstein\")\n",
    "                # choose threshold: prefer an explicit per-feature threshold in tbl_conf, else static\n",
    "                w_thresh = None\n",
    "                if isinstance(tbl_conf.get(\"feature_thresholds\", {}), dict):\n",
    "                    w_thresh = tbl_conf.get(\"feature_thresholds\", {}).get(resolved_col)\n",
    "                if w_thresh is None:\n",
    "                    # map to static threshold if present; default to 0.05 as conservative start\n",
    "                    w_thresh = float(self.static_thr.get(\"numeric_psi_max\", 0.20)) if self.static_thr else 0.20\n",
    "                drift_flag = False\n",
    "                triggered_metric = None\n",
    "                if w_val is not None and w_val >= float(w_thresh):\n",
    "                    drift_flag = True\n",
    "                    triggered_metric = \"wasserstein\"\n",
    "                else:\n",
    "                    # check median shift (use relative if present)\n",
    "                    med_rel = numeric_metrics.get(\"median_shift_rel\")\n",
    "                    med_abs = numeric_metrics.get(\"median_shift_abs\")\n",
    "                    med_thresh = float(self.static_thr.get(\"median_shift_pct\", 0.10)) if self.static_thr else 0.10\n",
    "                    if med_rel is not None and abs(med_rel) >= med_thresh:\n",
    "                        drift_flag = True\n",
    "                        triggered_metric = \"median_shift_rel\"\n",
    "                    elif med_abs is not None and abs(med_abs) >= med_thresh:\n",
    "                        drift_flag = True\n",
    "                        triggered_metric = \"median_shift_abs\"\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Error while deciding numeric drift for feature.\")\n",
    "                drift_flag = False\n",
    "                triggered_metric = None\n",
    "\n",
    "            # Emit numeric drift metric row\n",
    "            try:\n",
    "                metric_payload = {\n",
    "                    \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                    \"metric_type\": \"numeric_drift\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                    \"metric_value\": str(drift_flag), \"metric_value_num\": float(numeric_metrics.get(\"wasserstein\")) if numeric_metrics.get(\"wasserstein\") is not None else None,\n",
    "                    \"threshold\": str(w_thresh), \"reference_value\": float(numeric_metrics.get(\"q50_ref\")) if numeric_metrics.get(\"q50_ref\") is not None else None,\n",
    "                    \"status\": \"ALERT\" if drift_flag else \"PASS\", \"alert_flag\": \"Fail\" if drift_flag else \"Pass\", \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                }\n",
    "                self._emit_metric(metric_payload, partition_date=partition_date)\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Failed to emit numeric_drift metric.\")\n",
    "\n",
    "            # emit PSI diagnostic if available\n",
    "            if psi_val is not None:\n",
    "                try:\n",
    "                    psi_payload = {\n",
    "                        \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                        \"metric_type\": \"psi\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                        \"metric_value\": f\"{psi_val:.6f}\", \"metric_value_num\": float(psi_val), \"threshold\": f\"<={float(self.static_thr.get('numeric_psi_max', 0.20)):.6f}\",\n",
    "                        \"reference_value\": None, \"status\": \"PASS\" if psi_val <= float(self.static_thr.get(\"numeric_psi_max\", 0.20)) else \"FAIL\",\n",
    "                        \"alert_flag\": \"Fail\" if psi_val > float(self.static_thr.get(\"numeric_psi_max\", 0.20)) else None, \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                    }\n",
    "                    self._emit_metric(psi_payload, partition_date=partition_date)\n",
    "                except Exception:\n",
    "                    self.logger.exception(\"Failed to emit PSI metric.\")\n",
    "\n",
    "        # --------------------\n",
    "        # Categorical drift\n",
    "        # --------------------\n",
    "        categorical_cols_config = tbl_conf.get(\"categorical_cols\") or []\n",
    "        all_cols = tbl_conf.get(\"columns\", []) or []\n",
    "        date_cols_set = {c.lower() for c in (tbl_conf.get(\"date_columns\", {}) or {}).keys()}\n",
    "        numeric_drift_cols_set = {c.lower() for c in (numeric_drift_cols or [])}\n",
    "\n",
    "        cols_to_check_csi = set()\n",
    "        for c in categorical_cols_config:\n",
    "            if isinstance(c, str):\n",
    "                cols_to_check_csi.add(c)\n",
    "            elif isinstance(c, dict) and c.get(\"name\"):\n",
    "                cols_to_check_csi.add(c.get(\"name\"))\n",
    "        # also include columns not in numeric/date lists\n",
    "        for c in all_cols:\n",
    "            if c and c.lower() not in numeric_drift_cols_set and c.lower() not in date_cols_set:\n",
    "                cols_to_check_csi.add(c)\n",
    "\n",
    "        for col_name in cols_to_check_csi:\n",
    "            resolved_col = resolve_col_name(cur_df, col_name)\n",
    "            if not resolved_col:\n",
    "                self.logger.warning(f\"Categorical column '{col_name}' not found in {sys}.{tbl}.\")\n",
    "                self._mark_uncomputable(\"csi\", sys, tbl, col_name, partition_date=partition_date)\n",
    "                continue\n",
    "\n",
    "            chi2_val = None\n",
    "            cur_map = None\n",
    "            ref_map = None\n",
    "            # attempt compute via helper\n",
    "            try:\n",
    "                if \"compute_categorical_csi_and_buckets\" in globals():\n",
    "                    chi2_val, cur_map, ref_map = compute_categorical_csi_and_buckets(\n",
    "                        resolved_col, cur_df, ref_df,\n",
    "                        sample_size=int(self.static_thr.get(\"drift_sample_size\", DRIFT_SAMPLE_SIZE)),\n",
    "                        max_cardinality=int(self.static_thr.get(\"max_categorical_cardinality\", MAX_CATEGORICAL_CARDINALITY))\n",
    "                    )\n",
    "                else:\n",
    "                    chi2_val, cur_map, ref_map = None, None, None\n",
    "            except Exception:\n",
    "                self.logger.exception(f\"Error computing categorical buckets for {resolved_col}.\")\n",
    "\n",
    "            # emit current category snapshot if available\n",
    "            if cur_map is not None:\n",
    "                try:\n",
    "                    metric_value_buckets = json.dumps({\"categories\": list(cur_map.keys()), \"counts\": list(cur_map.values())}, default=str, ensure_ascii=False)\n",
    "                    self._emit_metric({\n",
    "                        \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                        \"metric_type\": \"csi_buckets\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                        \"metric_value\": metric_value_buckets, \"metric_value_num\": None, \"threshold\": None, \"reference_value\": None,\n",
    "                        \"status\": \"PASS\", \"alert_flag\": None, \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                    }, partition_date=partition_date)\n",
    "                except Exception:\n",
    "                    self.logger.exception(\"Failed to emit csi_buckets metric.\")\n",
    "            else:\n",
    "                self._mark_uncomputable(\"csi_buckets\", sys, tbl, resolved_col, partition_date=partition_date)\n",
    "\n",
    "            # fallback using historical csi_buckets if no ref and chi2 missing\n",
    "            if chi2_val is None and ref_df is None and mh_df is not None:\n",
    "                try:\n",
    "                    hist = mh_df.filter(\n",
    "                        (F.col(\"table_name\") == tbl) &\n",
    "                        (F.col(\"metric_type\") == \"csi_buckets\") &\n",
    "                        (F.col(\"column_name\") == resolved_col)\n",
    "                    ).orderBy(F.col(\"partition_date\").desc()).limit(MAX_LOOKBACK_SAMPLES).collect()\n",
    "                    for r in hist:\n",
    "                        parsed = _parse_buckets_from_metric_value(r[\"metric_value\"])\n",
    "                        if not parsed:\n",
    "                            continue\n",
    "                        fb_cats = parsed.get(\"categories\")\n",
    "                        fb_ref_counts = parsed.get(\"counts\")\n",
    "                        if not fb_cats or not fb_ref_counts or len(fb_cats) != len(fb_ref_counts):\n",
    "                            self.logger.warning(f\"Skipping historical CSI record due to invalid data: {r['partition_date']}\")\n",
    "                            continue\n",
    "                        try:\n",
    "                            ref_counts_map_fb = {}\n",
    "                            for i in range(len(fb_cats)):\n",
    "                                cat_key = fb_cats[i] if fb_cats[i] is not None else \"NULL\"\n",
    "                                ref_counts_map_fb[cat_key] = int(fb_ref_counts[i])\n",
    "\n",
    "                            # compute current counts aligned to historical categories\n",
    "                            if cur_map is None:\n",
    "                                try:\n",
    "                                    cur_rows_fb = cur_df.select(F.coalesce(F.col(resolved_col).cast(StringType()), F.lit(\"NULL\")).alias(resolved_col)) \\\n",
    "                                                       .groupBy(resolved_col).count().withColumnRenamed(\"count\", \"cur_cnt\").collect()\n",
    "                                    current_counts_map_fb = {row[resolved_col]: int(row[\"cur_cnt\"]) for row in cur_rows_fb}\n",
    "                                except Exception:\n",
    "                                    self.logger.exception(\"Failed to compute current category counts during CSI fallback.\")\n",
    "                                    continue\n",
    "                            else:\n",
    "                                current_counts_map_fb = cur_map\n",
    "\n",
    "                            chi2_fb = self._compute_chi2_from_maps(current_counts_map_fb, ref_counts_map_fb, EPSILON) if \"_compute_chi2_from_maps\" in dir(self) else _compute_chi2_from_maps(current_counts_map_fb, ref_counts_map_fb, EPSILON)\n",
    "                            if chi2_fb is not None:\n",
    "                                chi2_val = chi2_fb\n",
    "                                aligned_cats = sorted(list(set(current_counts_map_fb.keys()) | set(ref_counts_map_fb.keys())))\n",
    "                                cur_aligned_counts = [current_counts_map_fb.get(k, 0) for k in aligned_cats]\n",
    "                                metric_value_cur_fb = json.dumps({\"categories\": aligned_cats, \"counts\": cur_aligned_counts}, default=str, ensure_ascii=False)\n",
    "                                self._emit_metric({\n",
    "                                    \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                                    \"metric_type\": \"csi_buckets\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                                    \"metric_value\": metric_value_cur_fb, \"metric_value_num\": None, \"threshold\": None, \"reference_value\": None,\n",
    "                                    \"status\": \"PASS\", \"alert_flag\": None, \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                                }, partition_date=partition_date)\n",
    "                                self.logger.info(f\"Computed CSI using historical categories from {r['partition_date']}.\")\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            self.logger.exception(\"Error during CSI fallback using historical categories.\")\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    self.logger.exception(\"Error loading historical metrics for CSI fallback.\")\n",
    "\n",
    "            # Now compute primary categorical metrics: JS, entropy delta, top-k churn\n",
    "            cat_metrics = {}\n",
    "            try:\n",
    "                cur_map = cur_map or {}\n",
    "                ref_map = ref_map or {}\n",
    "                # if ref_map exists and cardinality acceptable, compute JS\n",
    "                cardinality = len(set(cur_map.keys()) | set(ref_map.keys()))\n",
    "                max_card = int(self.static_thr.get(\"max_categorical_cardinality\", MAX_CATEGORICAL_CARDINALITY)) if self.static_thr else MAX_CATEGORICAL_CARDINALITY\n",
    "                if ref_map and cardinality <= max_card:\n",
    "                    cat_metrics[\"js\"] = (_jensen_shannon_from_maps(cur_map, ref_map, top_k=50) if \"_jensen_shannon_from_maps\" not in dir(self) else (self._jensen_shannon_from_maps(cur_map, ref_map, top_k=50)))\n",
    "                    ent_cur = _entropy_from_map(cur_map)\n",
    "                    ent_ref = _entropy_from_map(ref_map)\n",
    "                    cat_metrics[\"entropy_cur\"] = ent_cur\n",
    "                    cat_metrics[\"entropy_ref\"] = ent_ref\n",
    "                    cat_metrics[\"entropy_delta\"] = ent_cur - ent_ref\n",
    "                    mass_delta, removed, added, churn = _topk_churn_and_mass_change(cur_map, ref_map, k=min(50, max_card))\n",
    "                    cat_metrics[\"topk_mass_delta\"] = mass_delta\n",
    "                    cat_metrics[\"topk_removed\"] = removed\n",
    "                    cat_metrics[\"topk_added\"] = added\n",
    "                    cat_metrics[\"topk_churn\"] = churn\n",
    "                else:\n",
    "                    # high-cardinality fallback: only top-k diagnostics (ref_map may be empty)\n",
    "                    mass_delta, removed, added, churn = _topk_churn_and_mass_change(cur_map, ref_map, k=50)\n",
    "                    cat_metrics[\"topk_mass_delta\"] = mass_delta\n",
    "                    cat_metrics[\"topk_removed\"] = removed\n",
    "                    cat_metrics[\"topk_added\"] = added\n",
    "                    cat_metrics[\"topk_churn\"] = churn\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Error assembling categorical metrics.\")\n",
    "\n",
    "            # Decide categorical drift using static thresholds\n",
    "            try:\n",
    "                js_val = cat_metrics.get(\"js\")\n",
    "                js_thresh = float(self.static_thr.get(\"categorical_csi_max\", 50.0)) if self.static_thr else 50.0\n",
    "                drift_flag_cat = False\n",
    "                triggered_metric_cat = None\n",
    "                if js_val is not None and js_val >= js_thresh:\n",
    "                    drift_flag_cat = True\n",
    "                    triggered_metric_cat = \"js\"\n",
    "                else:\n",
    "                    topk_mass = cat_metrics.get(\"topk_mass_delta\")\n",
    "                    topk_churn = cat_metrics.get(\"topk_churn\")\n",
    "                    topk_mass_thresh = float(self.static_thr.get(\"topk_mass_delta\", 0.05)) if self.static_thr else 0.05\n",
    "                    topk_churn_thresh = float(self.static_thr.get(\"topk_churn\", 0.3)) if self.static_thr else 0.3\n",
    "                    if topk_mass is not None and abs(topk_mass) >= topk_mass_thresh:\n",
    "                        drift_flag_cat = True\n",
    "                        triggered_metric_cat = \"topk_mass\"\n",
    "                    elif topk_churn is not None and topk_churn >= topk_churn_thresh:\n",
    "                        drift_flag_cat = True\n",
    "                        triggered_metric_cat = \"topk_churn\"\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Error while deciding categorical drift.\")\n",
    "                drift_flag_cat = False\n",
    "                triggered_metric_cat = None\n",
    "\n",
    "            # Emit categorical drift metric row\n",
    "            try:\n",
    "                metric_payload = {\n",
    "                    \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                    \"metric_type\": \"categorical_drift\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                    \"metric_value\": str(drift_flag_cat), \"metric_value_num\": float(cat_metrics.get(\"js\")) if cat_metrics.get(\"js\") is not None else None,\n",
    "                    \"threshold\": str(js_thresh) if 'js_thresh' in locals() else None, \"reference_value\": None,\n",
    "                    \"status\": \"ALERT\" if drift_flag_cat else \"PASS\", \"alert_flag\": \"Fail\" if drift_flag_cat else \"Pass\", \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                }\n",
    "                self._emit_metric(metric_payload, partition_date=partition_date)\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Failed to emit categorical_drift metric.\")\n",
    "\n",
    "            # emit chi2 diagnostic if available\n",
    "            if chi2_val is not None:\n",
    "                try:\n",
    "                    chi2_payload = {\n",
    "                        \"run_id\": RUN_ID, \"run_ts\": RUN_TS, \"partition_date\": partition_date,\n",
    "                        \"metric_type\": \"csi\", \"source_system\": sys, \"table_name\": tbl, \"column_name\": resolved_col,\n",
    "                        \"metric_value\": f\"{chi2_val:.6f}\", \"metric_value_num\": float(chi2_val),\n",
    "                        \"threshold\": f\"<={float(self.static_thr.get('categorical_csi_max', 50.0)):.6f}\",\n",
    "                        \"reference_value\": None, \"status\": \"PASS\" if chi2_val <= float(self.static_thr.get('categorical_csi_max', 50.0)) else \"FAIL\",\n",
    "                        \"alert_flag\": \"Fail\" if chi2_val > float(self.static_thr.get('categorical_csi_max', 50.0)) else None, \"country\": tbl_conf.get(\"country\", REGION)\n",
    "                    }\n",
    "                    self._emit_metric(chi2_payload, partition_date=partition_date)\n",
    "                except Exception:\n",
    "                    self.logger.exception(\"Failed to emit CSI (chi2) metric.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1758077753653,
  "creator": "2001810",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "modifiedBy": "2001810",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
