{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# DQ MONITOR — Core bootstrap, config, and thresholds\n",
    "# --------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "DQ Monitoring — quick start\n",
    "\n",
    "What this script does (at a glance)\n",
    "-----------------------------------\n",
    "- Loads Spark once per run and reuses cached tables during checks.\n",
    "- Writes both a consolidated monitoring report and a per-metric history table.\n",
    "- Uses recent metric history as fallback reference for PSI/CSI and drift baselines.\n",
    "- Captures internal exceptions into a dedicated dataset for auditability.\n",
    "\n",
    "Key outputs (Dataiku datasets)\n",
    "------------------------------\n",
    "- DQ_MONITORING_REPORT   : One row per metric/check evaluated in this run.\n",
    "- DQ_METRIC_HISTORY      : Long-term, per-metric event store (reference source for drift).\n",
    "- DQ_INTERNAL_ERRORS     : Fail-safe sink for internal exceptions (not data issues).\n",
    "\n",
    "How to run\n",
    "----------\n",
    "1) Ensure project flow variables include DKU_DST_date (YYYY-MM-DD). If absent, \"today\" is used.\n",
    "2) Provide region via project variables: region or country_region (defaults to 'hk').\n",
    "3) Run in Dataiku as a recipe or from a notebook with access to the same variables.\n",
    "\n",
    "Important knobs (tune as needed)\n",
    "--------------------------------\n",
    "- DRIFT_BINS                     : Histogram bins for PSI/CSI.\n",
    "- DRIFT_SAMPLE_SIZE              : Max sample size per partition for drift checks.\n",
    "- MAX_REF_AGE_DAYS               : Max age for reference partitions.\n",
    "- MAX_CATEGORICAL_CARDINALITY    : Guardrail for high-cardinality categorical features.\n",
    "- dq_static_thresholds           : Single source of truth for alert thresholds.\n",
    "\n",
    "Reliability notes\n",
    "-----------------\n",
    "- Empty-DF-safe operations (checks won’t explode on empty inputs).\n",
    "- No duplicate “raw metrics” — sanitization is applied before persisting.\n",
    "- Verbose logging with run_id and run_ts to tie all artifacts to a single execution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\"\"\"\n",
    "DQ monitoring script — canonical metrics_history, per-metric outputs derived.\n",
    "\"\"\"\n",
    "\n",
    "# --- Standard library ---\n",
    "import ast\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import uuid\n",
    "from datetime import date, datetime, timedelta\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# --- Dataiku and third-party ---\n",
    "import dataiku\n",
    "from dataiku import spark as dkuspark\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Pull in commonly used types so bare names (StructType, StringType, …) work\n",
    "from pyspark.sql.types import (\n",
    " StructType, StructField, StringType, DoubleType, TimestampType, NumericType, DateType\n",
    ")\n",
    "\n",
    "# Handy function aliases where code uses F_sum / F_countDistinct, etc.\n",
    "F_sum = F.sum\n",
    "F_countDistinct = F.countDistinct\n",
    "F_min = F.min\n",
    "F_max = F.max\n",
    "F_when = F.when\n",
    "\n",
    "# --- Internal ---\n",
    "from scmac.spark import start_spark_session\n",
    "from stables.data_sources.utils import get_ods_partition\n",
    "\n",
    "import json\n",
    "import math\n",
    "import uuid\n",
    "import logging\n",
    "from datetime import datetime, date, timedelta\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import dataiku\n",
    "from dataiku import spark as dkuspark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Constants / config\n",
    "# -----------------------\n",
    "\n",
    "DEFAULT_REGION = \"hk\" # Default region when not explicitly provided\n",
    "DRIFT_BINS = 10 # Number of bins for numeric drift (PSI/Wasserstein bucketization)\n",
    "EPSILON = 1e-9 # Small constant to avoid divide-by-zero/log(0) issues\n",
    "DRIFT_SAMPLE_SIZE = 15000 # Max rows sampled per partition for drift checks\n",
    "MAX_LOOKBACK_SAMPLES = 20 # Max historical partitions used for fallback PSI/CSI\n",
    "MAX_REF_AGE_DAYS = 10 # Reference partition must be within N days of current\n",
    "MAX_CATEGORICAL_CARDINALITY = 5000 # Upper cap on categories considered for CSI/JS diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Logger & Spark session\n",
    "# -----------------------\n",
    "logger = logging.getLogger(\"dq-monitor\")\n",
    "if not logger.handlers:\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n",
    "    logger.addHandler(h)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "#Spark Session\n",
    "spark: SparkSession\n",
    "if start_spark_session:\n",
    "    spark, sql_context = start_spark_session(\n",
    "        \"Mules_DQ_MONITOR\",\n",
    "        spark_config={\n",
    "            \"spark.executor.memory\": \"16g\",\n",
    "            \"spark.driver.memory\": \"16g\",\n",
    "            \"spark.sql.adaptive.enabled\": \"true\",\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "            \"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\": \"128m\",\n",
    "            \"spark.sql.broadcastTimeout\": \"1200\",\n",
    "            \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "            \"spark.dynamicAllocation.initialExecutors\": \"2\",\n",
    "            \"spark.dynamicAllocation.minExecutors\": \"1\",\n",
    "            \"spark.dynamicAllocation.maxExecutors\": \"10\",\n",
    "            \"spark.sql.ansi.enabled\": \"false\"\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    raise ImportError(\"start_spark_session not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- runtime / env ---\n",
    "try:\n",
    "    RUN_ID = str(uuid.uuid4())\n",
    "    RUN_TS = datetime.now().isoformat()\n",
    "    CURR_DATE_STR = dataiku.dku_flow_variables.get(\"DKU_DST_date\", datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    CURR_DATE = datetime.strptime(CURR_DATE_STR, \"%Y-%m-%d\").date()\n",
    "    REGION = dataiku.dku_flow_variables.get(\"region\", \"sg\")\n",
    "except Exception:\n",
    "    logger.warning(\"Flow vars missing; using defaults.\")\n",
    "    RUN_ID = str(uuid.uuid4())\n",
    "    RUN_TS = datetime.now().isoformat()\n",
    "    CURR_DATE_STR = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    CURR_DATE = datetime.now().date()\n",
    "    REGION = \"hk\"\n",
    "\n",
    "try:\n",
    "    _custom_vars = dataiku.get_custom_variables() or {}\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not retrieve custom variables: {e}. Using default values.\")\n",
    "    _custom_vars = {}\n",
    "\n",
    "REGION = (_custom_vars.get('country_region') or _custom_vars.get('region') or DEFAULT_REGION).lower()\n",
    "PROTEGRITY_UDF = _custom_vars.get('PROTEGRITY_UDF', 'protegrity.ptyProtectStr')\n",
    "PROTEGRITY_POLICY = _custom_vars.get('PROTEGRITY_POLICY', 'TE_A_N_L0R0_S23_Y_AST')\n",
    "RUN_TS = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "RUN_ID = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "PER_METRIC_COLUMNS_MAP: Dict[str, List[str]] = {\n",
    "    \"default\": [\"run_id\", \"run_ts\", \"partition_date\", \"metric_type\", \"source_system\", \"table_name\", \"column_name\", \"metric_value\", \"metric_value_num\", \"threshold\", \"reference_value\", \"status\", \"alert_flag\", \"country\"],\n",
    "    \"row_count\": [\"run_id\", \"run_ts\", \"partition_date\", \"metric_type\", \"source_system\", \"table_name\", \"metric_value\", \"metric_value_num\", \"reference_value\", \"status\", \"country\"],\n",
    "    \"psi_buckets\": [\"run_id\", \"run_ts\", \"partition_date\", \"metric_type\", \"source_system\", \"table_name\", \"column_name\", \"metric_value\", \"status\", \"country\"],\n",
    "    \"csi_buckets\": [\"run_id\", \"run_ts\", \"partition_date\", \"metric_type\", \"source_system\", \"table_name\", \"column_name\", \"metric_value\", \"status\", \"country\"],\n",
    "}\n",
    "\n",
    "OUTPUT_DATASETS: Dict[str, str] = {\n",
    "\"metrics_history_df\": \"DQ_METRIC_HISTORY\",\n",
    "\"monitoring_report_df\": \"DQ_MONITORING_REPORT\",\n",
    "\"internal_errors\": \"DQ_INTERNAL_ERRORS\",\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Thresholds\n",
    "# -----------------------\n",
    "\n",
    "dq_static_thresholds = {\n",
    " # --- core DQ ---\n",
    " \"row_count_drift_pct\": 0.01, # % deviation vs reference allowed for rowcount\n",
    " \"completeness_pct\": 99.0, # required non-null %\n",
    " \"uniqueness_pct\": 99.9, # required unique %\n",
    " \"join_consistency_pct\": 95.0, # min % rows that join successfully\n",
    " \"cross_system_key_consistency_pct\": 90.0,\n",
    " \"max_out_of_range_pct\": 0.0, # % values allowed outside declared ranges\n",
    " \"date_parse_min_pct\": 95.0, # % date parse success required\n",
    " \"latency_days\": 1, # freshness SLA\n",
    " \"max_ref_partition_age_days\": MAX_REF_AGE_DAYS, # cap on historical ref age\n",
    " \"historical_partition_window\": 30, # lookback window for dynamic rowcount calc (days)\n",
    "\n",
    " # --- drift: shared ---\n",
    " \"drift_sample_size\": DRIFT_SAMPLE_SIZE, # rows sampled for drift calcs\n",
    " \"drift_bins\": DRIFT_BINS, # numeric bin count for PSI/Wasserstein buckets\n",
    " \"max_categorical_cardinality\": MAX_CATEGORICAL_CARDINALITY, # cap for categorical diagnostics\n",
    "\n",
    " # --- numeric drift ---\n",
    " \"numeric_psi_max\": 0.20, # PSI diagnostic threshold (informational)\n",
    " \"median_shift_pct\": 0.10, # relative median shift trigger (|Δmedian| / |median_ref|)\n",
    " \"wasserstein_max\": 1.0, # ws drift threshold\n",
    "\n",
    " # --- categorical drift ---\n",
    " \"categorical_csi_max\": 50.0, # Chi-square diagnostic threshold (informational)\n",
    " \"js_divergence_max\": 1.15, # JS Shannon drift threshold (explicit)\n",
    " \"topk_mass_delta\": 0.05, # abs change in top-K mass to trigger drift\n",
    " \"topk_churn\": 0.30, # churn ratio in top-K categories to trigger drift\n",
    "}\n",
    "\n",
    "\n",
    "#system partition policy\n",
    "PARTITION_COL_BY_SYSTEM = {\n",
    "    \"TMX\": \"edmp_insert_date\",      # yyyy-MM-dd\n",
    "    \"ICM\": \"ods\",                   # yyyy-MM-dd\n",
    "    \"IBANKING\": \"ods\",              # yyyy-MM-dd\n",
    "    \"EBBS\": \"ods\",                  # yyyy-MM-dd  (feature dates like 'valuedt' are NOT the partition key)\n",
    "    \"HOGAN\": \"process_date\",        # yyyyMMdd for vw_casa_acct; most others 'ods' (yyyy-MM-dd). Table overrides win.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# How many days of daily-level partitions are retained by default\n",
    "DEFAULT_DAILY_RETENTION_DAYS = 60\n",
    "\n",
    "# System-level overrides (ICM is 45 as per your rule)\n",
    "DAILY_RETENTION_BY_SYSTEM = {\n",
    "    \"ICM\": 45,   # month-end only beyond 45 days\n",
    "    # add others if needed, else they inherit table/system or default\n",
    "}\n",
    "# we can still override per table with tbl_conf[\"daily_retention_days\"], or per system with sys_conf[\"daily_retention_days\"].\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Cross-system key harmonization (extracted)\n",
    "# -----------------------------\n",
    "cross_system_key_harmonization = [\n",
    " {\n",
    " \"description\": \"Mapping TMX account_login to IBANKING tkn_relationshipno via cust_ebid\",\n",
    " \"source_system_from\": \"TMX\",\n",
    " \"table_from\": \"vw_tmx_all_vrfy_usr_rspons\",\n",
    " \"key_from\": \"account_login\",\n",
    " \"source_system_to\": \"IBANKING\",\n",
    " \"table_to\": \"actv_tbl_cust\",\n",
    " \"key_to\": \"tkn_relationshipno\",\n",
    " \"common_linking_key_in_from\": \"account_login\",\n",
    " \"common_linking_key_in_to\": \"cust_ebid\",\n",
    " \"relationship_type\": \"account_to_token\"\n",
    " }\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Schemas\n",
    "# -----------------------\n",
    "metrics_history_schema = StructType([\n",
    "    T.StructField(\"run_id\", T.StringType(), False),\n",
    "    T.StructField(\"run_ts\", T.StringType(), False),\n",
    "    T.StructField(\"partition_date\", T.StringType(), False),\n",
    "    T.StructField(\"metric_type\", T.StringType(), False),\n",
    "    T.StructField(\"source_system\", T.StringType(), True),\n",
    "    T.StructField(\"table_name\", T.StringType(), True),\n",
    "    T.StructField(\"column_name\", T.StringType(), True),\n",
    "    T.StructField(\"metric_value\", T.StringType(), True),\n",
    "    T.StructField(\"metric_value_num\", T.DoubleType(), True),\n",
    "    T.StructField(\"threshold\", T.StringType(), True),\n",
    "    T.StructField(\"reference_value\", T.DoubleType(), True),\n",
    "    T.StructField(\"status\", T.StringType(), False),\n",
    "    T.StructField(\"alert_flag\", T.StringType(), True),\n",
    "    T.StructField(\"country\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "monitoring_report_schema = StructType([\n",
    "    T.StructField(\"run_id\", T.StringType(), False),\n",
    "    T.StructField(\"run_ts\", T.StringType(), False),\n",
    "    T.StructField(\"partition_date\", T.StringType(), False),\n",
    "    T.StructField(\"metric_type\", T.StringType(), False),\n",
    "    T.StructField(\"source_system\", T.StringType(), True),\n",
    "    T.StructField(\"table_name\", T.StringType(), True),\n",
    "    T.StructField(\"column_name\", T.StringType(), True),\n",
    "    T.StructField(\"alert_flag\", T.StringType(), True),\n",
    "    T.StructField(\"metric_value\", T.StringType(), True),\n",
    "    T.StructField(\"metric_value_num\", T.DoubleType(), True),\n",
    "    T.StructField(\"threshold\", T.StringType(), True),\n",
    "    T.StructField(\"reference_value\",T.DoubleType(), True),\n",
    "    T.StructField(\"status\", T.StringType(), False),\n",
    "    T.StructField(\"country\", T.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Table Configurations\n",
    "# -----------------------\n",
    "\n",
    "def validate_and_normalize_table_config(raw_conf: Dict[str, Dict[str, Any]], region: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Normalize the raw table configuration:\n",
    "      - convert legacy 'columns' lists into per-column metadata dicts\n",
    "      - infer roles from hints (numeric_drift_cols, date_columns, join_key)\n",
    "      - populate preprocess/cast_to defaults for numeric/date features\n",
    "      - set per-table country/region defaults based on source_system and provided region\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, Any]] = {}\n",
    "    reg_norm = (region or \"hk\").strip().lower()\n",
    "\n",
    "    for sys_name, sys_conf in (raw_conf or {}).items():\n",
    "        sys_copy = copy.deepcopy(sys_conf)  # do not mutate input\n",
    "        sys_copy.setdefault(\"default_partition_col\", sys_conf.get(\"default_partition_col\", \"ods\"))\n",
    "\n",
    "        tables = sys_copy.get(\"tables\", {})\n",
    "        normalized_tables: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        for tbl_name, tbl_conf in tables.items():\n",
    "            tc = copy.deepcopy(tbl_conf)  # table copy\n",
    "\n",
    "            if not tc.get(\"full_table_path\"):\n",
    "                schema = tc.get(\"schema\")\n",
    "                if schema:\n",
    "                    tc[\"full_table_path\"] = f\"{schema}.{tbl_name}\"\n",
    "                else:\n",
    "                    tc[\"full_table_path\"] = tbl_name\n",
    "                    logger.warning(f\"{sys_name}.{tbl_name} missing full_table_path and schema; using table name '{tbl_name}'\")\n",
    "\n",
    "            cols = tc.get(\"columns\") or []\n",
    "            col_map: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "            if isinstance(cols, list):\n",
    "                for c in cols:\n",
    "                    if isinstance(c, str):\n",
    "                        col_map[c] = {\n",
    "                            \"role\": None, \"data_type\": None, \"cast_to\": None, \"preprocess\": {},\n",
    "                            \"sensitive\": False, \"tokenize\": False, \"tokenize_policy\": None,\n",
    "                            \"numerical_range\": None, \"thresholds\": {}, \"cardinality_hint\": None,\n",
    "                            \"skip_checks\": []\n",
    "                        }\n",
    "                    elif isinstance(c, dict) and c.get(\"name\"):\n",
    "                        name = c[\"name\"]\n",
    "                        col_map[name] = {\n",
    "                            \"role\": c.get(\"role\"), \"data_type\": c.get(\"data_type\"), \"cast_to\": c.get(\"cast_to\"),\n",
    "                            \"preprocess\": c.get(\"preprocess\", {}), \"sensitive\": c.get(\"sensitive\", False),\n",
    "                            \"tokenize\": c.get(\"tokenize\", False), \"tokenize_policy\": c.get(\"tokenize_policy\"),\n",
    "                            \"numerical_range\": c.get(\"numerical_range\", None) or tc.get(\"numerical_ranges\", {}).get(name),\n",
    "                            \"thresholds\": c.get(\"thresholds\", {}), \"cardinality_hint\": c.get(\"cardinality_hint\"),\n",
    "                            \"skip_checks\": c.get(\"skip_checks\", []),\n",
    "                        }\n",
    "                    else:\n",
    "                        logger.warning(f\"Unrecognized column entry for {sys_name}.{tbl_name}: {c}\")\n",
    "            elif isinstance(cols, dict):\n",
    "                for name, meta in cols.items():\n",
    "                    col_map[name] = {\n",
    "                        \"role\": meta.get(\"role\"), \"data_type\": meta.get(\"data_type\"), \"cast_to\": meta.get(\"cast_to\"),\n",
    "                        \"preprocess\": meta.get(\"preprocess\", {}), \"sensitive\": meta.get(\"sensitive\", False),\n",
    "                        \"tokenize\": meta.get(\"tokenize\", False), \"tokenize_policy\": meta.get(\"tokenize_policy\"),\n",
    "                        \"numerical_range\": meta.get(\"numerical_range\", None) or tc.get(\"numerical_ranges\", {}).get(name),\n",
    "                        \"thresholds\": meta.get(\"thresholds\", {}), \"cardinality_hint\": meta.get(\"cardinality_hint\"),\n",
    "                        \"skip_checks\": meta.get(\"skip_checks\", [])\n",
    "                    }\n",
    "\n",
    "            for nm in (tc.get(\"numeric_drift_cols\") or []):\n",
    "                if nm not in col_map:\n",
    "                    col_map[nm] = {\n",
    "                        \"role\": \"numeric_feature\", \"data_type\": None, \"cast_to\": \"double\",\n",
    "                        \"preprocess\": {\"strip_commas\": True, \"trim\": True}, \"sensitive\": False,\n",
    "                        \"tokenize\": False, \"tokenize_policy\": None,\n",
    "                        \"numerical_range\": tc.get(\"numerical_ranges\", {}).get(nm), \"thresholds\": {},\n",
    "                        \"cardinality_hint\": None, \"skip_checks\": []\n",
    "                    }\n",
    "                else:\n",
    "                    col_map[nm][\"role\"] = col_map[nm].get(\"role\") or \"numeric_feature\"\n",
    "                    col_map[nm].setdefault(\"cast_to\", \"double\")\n",
    "                    col_map[nm].setdefault(\"preprocess\", {\"strip_commas\": True, \"trim\": True})\n",
    "\n",
    "            for nm, fmt in (tc.get(\"date_columns\") or {}).items():\n",
    "                if nm not in col_map:\n",
    "                    col_map[nm] = {\n",
    "                        \"role\": \"timestamp\", \"data_type\": \"timestamp\", \"cast_to\": \"timestamp\",\n",
    "                        \"preprocess\": {}, \"sensitive\": False, \"tokenize\": False, \"tokenize_policy\": None,\n",
    "                        \"numerical_range\": None, \"thresholds\": {}, \"cardinality_hint\": None,\n",
    "                        \"skip_checks\": []\n",
    "                    }\n",
    "                else:\n",
    "                    col_map[nm][\"role\"] = col_map[nm].get(\"role\") or \"timestamp\"\n",
    "                    col_map[nm].setdefault(\"cast_to\", \"timestamp\")\n",
    "\n",
    "            jk = tc.get(\"join_key\")\n",
    "            if jk:\n",
    "                if jk not in col_map:\n",
    "                    col_map[jk] = {\n",
    "                        \"role\": \"primary_key\", \"data_type\": None, \"cast_to\": None,\n",
    "                        \"preprocess\": {}, \"sensitive\": False, \"tokenize\": False, \"tokenize_policy\": None,\n",
    "                        \"numerical_range\": None, \"thresholds\": {}, \"cardinality_hint\": None,\n",
    "                        \"skip_checks\": []\n",
    "                    }\n",
    "                else:\n",
    "                    col_map[jk][\"role\"] = col_map[jk].get(\"role\") or \"primary_key\"\n",
    "\n",
    "            for candidate in (tc.get(\"tokenize_candidates\") or []):\n",
    "                if candidate in col_map:\n",
    "                    col_map[candidate][\"tokenize\"] = True\n",
    "                    col_map[candidate][\"sensitive\"] = True\n",
    "                    col_map[candidate].setdefault(\"tokenize_policy\", PROTEGRITY_POLICY)\n",
    "\n",
    "            for coln, meta in list(col_map.items()):\n",
    "                if not meta.get(\"role\"):\n",
    "                    lname = coln.lower()\n",
    "                    if lname.endswith(\"_id\") or \"id\" in lname or lname.endswith(\"no\") or lname.endswith(\"num\"):\n",
    "                        meta[\"role\"] = \"identifier\"\n",
    "                    elif any(x in lname for x in [\"date\", \"dt\", \"timestamp\", \"ts\"]):\n",
    "                        meta[\"role\"] = \"timestamp\"\n",
    "                    elif any(x in lname for x in [\"amt\", \"amount\", \"bal\", \"score\", \"limit\", \"sum\",\"rate\"]):\n",
    "                        meta[\"role\"] = \"numeric_feature\"\n",
    "                        meta.setdefault(\"cast_to\", \"double\")\n",
    "                        meta.setdefault(\"preprocess\", {\"strip_commas\": True, \"trim\": True})\n",
    "                    elif any(x in lname for x in [\"code\", \"type\", \"category\", \"flag\", \"status\", \"ccy\", \"country\", \"currency\"]):\n",
    "                        meta[\"role\"] = \"categorical_feature\"\n",
    "                    else:\n",
    "                        meta[\"role\"] = \"categorical_feature\"\n",
    "\n",
    "                if meta.get(\"cardinality_hint\") in (None, \"auto\"):\n",
    "                    meta[\"cardinality_hint\"] = meta.get(\"cardinality_hint\")\n",
    "\n",
    "                meta[\"skip_checks\"] = meta.get(\"skip_checks\") or []\n",
    "                meta[\"thresholds\"] = meta.get(\"thresholds\") or {}\n",
    "\n",
    "                if not meta.get(\"numerical_range\"):\n",
    "                    meta[\"numerical_range\"] = tc.get(\"numerical_ranges\", {}).get(coln)\n",
    "\n",
    "                col_map[coln] = meta\n",
    "\n",
    "            tc[\"columns\"] = col_map\n",
    "\n",
    "            tc.setdefault(\"feature_thresholds\", tc.get(\"feature_thresholds\", {}))\n",
    "            tc.setdefault(\"tokenize_candidates\", tc.get(\"tokenize_candidates\", []))\n",
    "\n",
    "            src = (tc.get(\"source_system\") or sys_name).strip().upper()\n",
    "            if src == \"HOGAN\":\n",
    "                tc.setdefault(\"country\", \"hk\")\n",
    "                tc.setdefault(\"region\", \"hk\")\n",
    "            elif src == \"EBBS\":\n",
    "                tc.setdefault(\"country\", \"sg\")\n",
    "                tc.setdefault(\"region\", \"sg\")\n",
    "            elif src in (\"ICM\", \"TMX\", \"IBANKING\"):\n",
    "                tc.setdefault(\"country\", reg_norm)\n",
    "                tc.setdefault(\"region\", reg_norm)\n",
    "                tc.setdefault(\"region_hint\", \"hk/sg\")\n",
    "            else:\n",
    "                tc.setdefault(\"country\", reg_norm)\n",
    "                tc.setdefault(\"region\", reg_norm)\n",
    "                tc.setdefault(\"region_hint\", \"sg/hk\")\n",
    "\n",
    "            normalized_tables[tbl_name] = tc\n",
    "\n",
    "        sys_copy[\"tables\"] = normalized_tables\n",
    "        out[sys_name] = sys_copy\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_config(region: Optional[str]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build raw table configuration and then normalize it with validate_and_normalize_table_config.\n",
    "    \"\"\"\n",
    "    region_code = (region or \"hk\").strip().lower()\n",
    "\n",
    "    hog_schema = f\"prd_vw_sri_hog_{region_code}_sen\"\n",
    "    hog_adm_schema = f\"prd_vw_adm_{region_code}_nsen\"\n",
    "    ebbs_schema = f\"prd_vw_sri_ebbs_{region_code}_sen\"\n",
    "    icm_schema = f\"prd_vw_sri_icm_{region_code}_sen\"\n",
    "    ibanking_schema = f\"prd_vw_sri_ibanking_{region_code}_tkn\"\n",
    "    tmx_schema = f\"prd_vw_fdl_tmx_{region_code}_sen\" if region_code == \"sg\" else f\"prd_vw_sri_tmx_{region_code}_sen\"\n",
    "\n",
    "    raw = {\n",
    "    \"HOGAN\": {\n",
    "        \"default_partition_col\": \"ods\",\n",
    "        \"tables\": {\n",
    "            \"actv_cst_data\": {\n",
    "                \"full_table_path\": f\"{hog_schema}.actv_cst_data\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"cust_key\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"cust_key\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"cust_id_type\", \"role\": \"categorical_feature\"},\n",
    "                    # Treat ID as sensitive + exclude from CSI drift\n",
    "                    {\"name\": \"cust_id_no\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"cust_csi\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"actv_amt_tda_acct\": {\n",
    "                \"full_table_path\": f\"{hog_schema}.actv_amt_tda_acct\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"acct_num\",\n",
    "                \"timestamp_col\": \"act_opend_dt\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"prmry_cust_key\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"acct_num\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"act_opend_dt\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"act_clsed_dt\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"act_st\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"act_prod\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"act_to_cust_rel\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"act_opend_dt\": \"yyyy-MM-dd\", \"act_clsed_dt\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "            \"actv_gaml_txn_cb\": {\n",
    "                \"full_table_path\": f\"{hog_schema}.actv_gaml_txn_cb\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"timestamp_col\": \"d_eff_dt\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"d_acct_num\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"d_tran_ccy\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"d_eff_dt\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"d_tran_type\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"d_tran_amt\", \"role\": \"numeric_feature\", \"data_type\": \"decimal\", \"cast_to\": \"double\",\n",
    "                     \"preprocess\": {\"strip_commas\": True, \"trim\": True}, \"thresholds\": {\"wasserstein\": 0.05}},\n",
    "                    {\"name\": \"d_tran_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"d_tran_desc\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"d_matched_rev\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"d_eff_dt\": \"yyyy-MM-dd\"},\n",
    "                \"numeric_drift_cols\": [\"d_tran_amt\"],\n",
    "            },\n",
    "            \"actv_tran_code\": {\n",
    "                \"full_table_path\": f\"{hog_schema}.actv_tran_code\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"txn_code\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"txn_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"desc1\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"vw_casa_acct\": {\n",
    "                \"full_table_path\": f\"{hog_adm_schema}.vw_casa_acct\",\n",
    "                \"partition_col\": \"process_date\",\n",
    "                \"join_key\": \"n_acct\",\n",
    "                \"timestamp_col\": \"process_date\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"n_acct\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"c_acctccy\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"a_curbal_acctccy\", \"role\": \"numeric_feature\", \"cast_to\": \"double\", \"preprocess\": {\"strip_commas\": True}},\n",
    "                    {\"name\": \"process_date\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"process_date\": \"yyyyMMdd\"},\n",
    "            },\n",
    "            \"actv_grouprates\": {\n",
    "                \"full_table_path\": \"prd_vw_sri_act_grp_tkn.actv_grouprates\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"sc_cury\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"sc_cury\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"sc_cury_2\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"sc_cash\", \"role\": \"numeric_feature\", \"cast_to\": \"double\", \"preprocess\": {\"strip_commas\": True}},\n",
    "                    {\"name\": \"ods\", \"role\": \"timestamp\"},\n",
    "                ],\n",
    "                \"numeric_drift_cols\": [\"sc_cash\"],\n",
    "            },\n",
    "        },\n",
    "        \"tables_to_join_for_consistency\": [\n",
    "            {\"left_table\": \"actv_amt_tda_acct\", \"right_table\": \"actv_cst_data\", \"join_keys\": [\"prmry_cust_key\"]},\n",
    "            {\"left_table\": \"actv_gaml_txn_cb\", \"right_table\": \"actv_tran_code\", \"join_keys\": [\"d_tran_code\"]},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"EBBS\": {\n",
    "        \"default_partition_col\": \"ods\",\n",
    "        \"tables\": {\n",
    "            \"actv_mastrel\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_mastrel\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"masterno\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"masterno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"relationshipno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"custsegmtcode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"primaryflag\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"actv_rel\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_rel\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"relationshipno\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"relationshipno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"custsegmtcode\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"actv_account\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_account\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"accountno\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"masterno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"accountno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"acopendate\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"acctcurrentstatus\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"productcode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"acclosedt\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"acopendate\": \"yyyy-MM-dd\", \"acclosedt\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "            \"actv_currrt\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_currrt\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"currencycode\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"currencycode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"midrate\", \"role\": \"numeric_feature\", \"cast_to\": \"double\"},\n",
    "                ],\n",
    "                \"numeric_drift_cols\": [\"midrate\"],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"actv_bebal\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_bebal\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"accountno\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"accountno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"currencycode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"openingbalance\", \"role\": \"numeric_feature\", \"cast_to\": \"double\", \"preprocess\": {\"strip_commas\": True}},\n",
    "                    {\"name\": \"date1\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"date1\": \"yyyy-MM-dd\"},\n",
    "                \"numeric_drift_cols\": [\"openingbalance\"],\n",
    "            },\n",
    "            \"actv_trnarc\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_trnarc\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"trncode\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"accountno\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"currencycode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"valuedt\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"creditdebit\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"channelid\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"transactionamount\", \"role\": \"numeric_feature\", \"cast_to\": \"double\", \"preprocess\": {\"strip_commas\": True}},\n",
    "                    {\"name\": \"trncode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"transtypecode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"NARRATION1\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"reversalflag\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"financialtrnflag\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"valuedt\": \"yyyy-MM-dd\"},\n",
    "                \"numeric_drift_cols\": [\"transactionamount\"],\n",
    "            },\n",
    "            \"actv_trncd\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_trncd\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"trncode\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"trncode\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"description\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"actv_chnl\": {\n",
    "                \"full_table_path\": f\"{ebbs_schema}.actv_chnl\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"channelid\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"channelid\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"name\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "        },\n",
    "        \"tables_to_join_for_consistency\": [\n",
    "            {\"left_table\": \"actv_mastrel\", \"right_table\": \"actv_account\", \"join_keys\": [\"masterno\"]},\n",
    "            {\"left_table\": \"actv_trnarc\", \"right_table\": \"actv_trncd\", \"join_keys\": [\"trncode\"]},\n",
    "            {\"left_table\": \"actv_trnarc\", \"right_table\": \"actv_chnl\", \"join_keys\": [\"channelid\"]},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"ICM\": {\n",
    "        \"default_partition_col\": \"ods\",\n",
    "        \"tables\": {\n",
    "            \"actv_cust_profiles\": {\n",
    "                \"full_table_path\": f\"{icm_schema}.actv_cust_profiles\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"profile_id\",\n",
    "                \"tokenize_candidates\": [\"relationship_number\"],\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"profile_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"relationship_number\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"date_of_birth\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"country_of_birth\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"qualification_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"resident_country\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"nationality_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"full_name\", \"role\": \"categorical_feature\"},\n",
    "                    # partitioned by ods (date); also track creation date as timestamp\n",
    "                    {\"name\": \"profile_create_date\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"profile_create_date\": \"yyyy-MM-dd\", \"date_of_birth\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "            \"actv_cust_risk_indicators\": {\n",
    "                \"full_table_path\": f\"{icm_schema}.actv_cust_risk_indicators\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"profile_id\",\n",
    "                \"timestamp_col\": \"risk_start_date\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"profile_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"risk_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"risk_start_date\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"risk_start_date\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "            \"actv_cust_profile_business\": {\n",
    "                \"full_table_path\": f\"{icm_schema}.actv_cust_profile_business\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"profile_id\",\n",
    "                \"timestamp_col\": \"profile_create_date\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"profile_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"rel_status\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"profile_create_date\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"profile_create_date\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "            \"actv_cust_product_references\": {\n",
    "                \"full_table_path\": f\"{icm_schema}.actv_cust_product_references\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"profile_id\",\n",
    "                \"timestamp_col\": \"product_open_date\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"profile_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"product_reference_number\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"product_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"sub_product_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"product_reference_status\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"product_contact_type\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"product_open_date\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"product_open_date\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "            \"actv_cust_employments\": {\n",
    "                \"full_table_path\": f\"{icm_schema}.actv_cust_employments\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"profile_id\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"profile_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"profession_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"employer_name\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"own_organisation_name\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"emp_banking_indicator\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"actv_cust_contacts\": {\n",
    "                \"full_table_path\": f\"{icm_schema}.actv_cust_contacts\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"profile_id\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"profile_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"contact_classification_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"contact_country_code\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"contact_area_code\", \"role\": \"categorical_feature\"},\n",
    "                    # treat phone-like data as sensitive identifiers; exclude from CSI\n",
    "                    {\"name\": \"contact_number\", \"role\": \"identifier\", \"tokenize\": True, \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"contact_extension\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"contact_reference\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"primary_contact\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"s_startdt\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"s_startdt\": \"yyyy-MM-dd\"},\n",
    "            },\n",
    "        },\n",
    "        \"tables_to_join_for_consistency\": [\n",
    "            {\"left_table\": \"actv_cust_profiles\", \"right_table\": \"actv_cust_profile_business\", \"join_keys\": [\"profile_id\"]},\n",
    "            {\"left_table\": \"actv_cust_profiles\", \"right_table\": \"actv_cust_product_references\", \"join_keys\": [\"profile_id\"]},\n",
    "            {\"left_table\": \"actv_cust_profiles\", \"right_table\": \"actv_cust_risk_indicators\", \"join_keys\": [\"profile_id\"]},\n",
    "            {\"left_table\": \"actv_cust_profiles\", \"right_table\": \"actv_cust_employments\", \"join_keys\": [\"profile_id\"]},\n",
    "            {\"left_table\": \"actv_cust_profiles\", \"right_table\": \"actv_cust_contacts\", \"join_keys\": [\"profile_id\"]},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"TMX\": {\n",
    "        \"default_partition_col\": \"edmp_insert_date\",\n",
    "        \"tables\": {\n",
    "            \"vw_tmx_all_vrfy_usr_rspons\": {\n",
    "                \"full_table_path\": f\"{tmx_schema}.vw_tmx_all_vrfy_usr_rspons\",\n",
    "                \"partition_col\": \"edmp_insert_date\",\n",
    "                \"join_key\": \"request_id\",\n",
    "                \"timestamp_col\": \"edmp_insert_timestamp\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"request_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"edmp_insert_date\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"edmp_insert_timestamp\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                    {\"name\": \"account_login\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"fuzzy_device_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"fuzzy_device_score\", \"role\": \"numeric_feature\", \"cast_to\": \"double\"},\n",
    "                    {\"name\": \"agent_type\", \"role\": \"categorical_feature\"},\n",
    "                    {\"name\": \"input_ip_address\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"event_type\", \"role\": \"categorical_feature\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"edmp_insert_date\": \"yyyy-MM-dd\", \"edmp_insert_timestamp\": \"yyyy-MM-dd HH:mm:ss\"},\n",
    "            },\n",
    "            \"vw_tmx_all_vrfy_usr_rspons_ext1\": {\n",
    "                \"full_table_path\": f\"{tmx_schema}.vw_tmx_all_vrfy_usr_rspons_ext1\",\n",
    "                \"partition_col\": \"edmp_insert_date\",\n",
    "                \"join_key\": \"request_id\",\n",
    "                \"numerical_ranges\": {\n",
    "                    \"summary_risk_score\": {\"min\": 0, \"max\": 100},\n",
    "                    \"transaction_amount\": {\"min\": 0, \"max\": 100_000_000}\n",
    "                },\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"request_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"summary_risk_score\", \"role\": \"numeric_feature\", \"cast_to\": \"double\"},\n",
    "                    {\"name\": \"transaction_amount_usd\", \"role\": \"numeric_feature\", \"cast_to\": \"double\",\n",
    "                     \"preprocess\": {\"strip_commas\": True}},\n",
    "                ],\n",
    "                \"numeric_drift_cols\": [\"transaction_amount_usd\"],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "            \"vw_tmx_all_vrfy_usr_rspons_ext2\": {\n",
    "                \"full_table_path\": f\"{tmx_schema}.vw_tmx_all_vrfy_usr_rspons_ext2\",\n",
    "                \"partition_col\": \"edmp_insert_date\",\n",
    "                \"join_key\": \"request_id\",\n",
    "                \"timestamp_col\": \"event_datetime\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"request_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"vpn_score\", \"role\": \"numeric_feature\", \"cast_to\": \"double\"},\n",
    "                    {\"name\": \"transaction_amount_usd\", \"role\": \"numeric_feature\", \"cast_to\": \"double\",\n",
    "                     \"preprocess\": {\"strip_commas\": True}},\n",
    "                    {\"name\": \"event_datetime\", \"role\": \"timestamp\", \"cast_to\": \"timestamp\"},\n",
    "                ],\n",
    "                \"date_columns\": {\"event_datetime\": \"yyyy-MM-dd HH:mm:ss\"},\n",
    "                \"numeric_drift_cols\": [\"transaction_amount_usd\", \"vpn_score\"],\n",
    "            },\n",
    "            \"vw_tmx_all_vrfy_usr_rspons_ext3\": {\n",
    "                \"full_table_path\": f\"{tmx_schema}.vw_tmx_all_vrfy_usr_rspons_ext3\",\n",
    "                \"partition_col\": \"edmp_insert_date\",\n",
    "                \"join_key\": \"request_id\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"request_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"account_customer_id\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            },\n",
    "        },\n",
    "        \"tables_to_join_for_consistency\": [\n",
    "            {\"left_table\": \"vw_tmx_all_vrfy_usr_rspons\", \"right_table\": \"vw_tmx_all_vrfy_usr_rspons_ext1\", \"join_keys\": [\"request_id\"]},\n",
    "            {\"left_table\": \"vw_tmx_all_vrfy_usr_rspons\", \"right_table\": \"vw_tmx_all_vrfy_usr_rspons_ext2\", \"join_keys\": [\"request_id\"]},\n",
    "            {\"left_table\": \"vw_tmx_all_vrfy_usr_rspons\", \"right_table\": \"vw_tmx_all_vrfy_usr_rspons_ext3\", \"join_keys\": [\"request_id\"]},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"IBANKING\": {\n",
    "        \"default_partition_col\": \"ods\",\n",
    "        \"tables\": {\n",
    "            \"actv_tbl_cust\": {\n",
    "                \"full_table_path\": f\"{ibanking_schema}.actv_tbl_cust\",\n",
    "                \"partition_col\": \"ods\",\n",
    "                \"join_key\": \"cust_ebid\",\n",
    "                \"columns\": [\n",
    "                    {\"name\": \"cust_ebid\", \"role\": \"identifier\", \"skip_checks\": [\"csi\"]},\n",
    "                    {\"name\": \"tkn_relationshipno\", \"role\": \"identifier\", \"tokenize\": True,\n",
    "                     \"tokenize_policy\": PROTEGRITY_POLICY, \"skip_checks\": [\"csi\"]},\n",
    "                ],\n",
    "                \"date_columns\": {},\n",
    "            }\n",
    "        },\n",
    "        \"tables_to_join_for_consistency\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "    return validate_and_normalize_table_config(raw_conf=raw, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def df_is_empty(df: Optional[DataFrame]) -> bool:\n",
    "    \"\"\"Robust empty check for none or no rows.\"\"\"\n",
    "    if df is None:\n",
    "        return True\n",
    "    try:\n",
    "        return df.count() == 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking if DataFrame is empty: {e}\")\n",
    "        # If we can't even count it, assume it's effectively empty or problematic.\n",
    "        return True\n",
    "\n",
    "def write_dataset(dataset_name: str, df: Optional[DataFrame], mode: str = \"append\", **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Write a DataFrame to a Dataiku dataset.\n",
    "    - Preferred: mode in {\"append\",\"overwrite\"}\n",
    "    - Back-compat: append=True/False (will be mapped to mode)\n",
    "    \"\"\"\n",
    "    if df is None or df_is_empty(df):\n",
    "        logger.info(f\"Skipping write to dataset '{dataset_name}': DataFrame is None or empty.\")\n",
    "        return\n",
    "\n",
    "    if \"append\" in kwargs and kwargs[\"append\"] is not None:\n",
    "        mode = \"append\" if bool(kwargs[\"append\"]) else \"overwrite\"\n",
    "\n",
    "    try:\n",
    "        ds = dataiku.Dataset(dataset_name)\n",
    "        dkuspark.write_with_schema(ds, df, options={\"mode\": mode})\n",
    "        logger.info(f\"Wrote to '{dataset_name}' (mode={mode}).\")\n",
    "    except Exception:\n",
    "        logger.exception(f\"Failed to write Dataiku dataset '{dataset_name}'.\")\n",
    "\n",
    "\n",
    "def _lower_map(cols: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Creates a mapping from lowercase column name to original column name.\"\"\"\n",
    "    lm: Dict[str, str] = {}\n",
    "    for c in cols:\n",
    "        lc = c.lower()\n",
    "        if lc in lm:\n",
    "            raise ValueError(f\"Duplicate case-insensitive column: '{lc}' -> {lm[lc]} and {c}\")\n",
    "        lm[lc] = c\n",
    "    return lm\n",
    "\n",
    "\n",
    "def resolve_col_name(df: DataFrame, name: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Resolves a column name, ignoring case, using a cached mapping.\"\"\"\n",
    "    if not name or df is None:\n",
    "        return None\n",
    "    # Ensure lower_map is generated only once per DataFrame or cached.\n",
    "    # For simplicity here, it's generated each call, assuming schema doesn't change mid-function.\n",
    "    lm = _lower_map(df.columns)\n",
    "    return lm.get(name.lower())\n",
    "\n",
    "def resolve_col_list(df: DataFrame, cols: List[str]) -> List[str]:\n",
    "    \"\"\"Resolves a list of column names, ignoring case.\"\"\"\n",
    "    if df is None:\n",
    "        return []\n",
    "    lm = _lower_map(df.columns)\n",
    "    resolved: List[str] = []\n",
    "    for c in cols:\n",
    "        r = lm.get(c.lower())\n",
    "        if r:\n",
    "            resolved.append(r)\n",
    "        else:\n",
    "            logger.warning(f\"Column '{c}' not found in DataFrame schema.\")\n",
    "    return resolved\n",
    "\n",
    "def _system_partition_col(sys_name: str, tbl_conf: Dict[str, Any]) -> str:\n",
    "    # Table override > table default_partition_col > system default > 'ods'\n",
    "    return (\n",
    "        tbl_conf.get(\"partition_col\")\n",
    "        or tbl_conf.get(\"default_partition_col\")\n",
    "        or PARTITION_COL_BY_SYSTEM.get((sys_name or \"\").upper(), \"ods\")\n",
    "    )\n",
    "\n",
    "\n",
    "def rows_to_df(spark_sess: SparkSession, rows: List[Dict[str, Any]], schema: Optional[StructType] = None) -> Optional[DataFrame]:\n",
    "    \"\"\"Creates a Spark DataFrame from a list of dictionaries.\"\"\"\n",
    "    if not rows:\n",
    "        if schema is not None:\n",
    "            return spark_sess.createDataFrame([], schema=schema)\n",
    "        return None\n",
    "    try:\n",
    "        if schema is not None:\n",
    "            return spark_sess.createDataFrame(rows, schema=schema)\n",
    "        return spark_sess.createDataFrame(rows)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to create DataFrame from rows. Schema provided: {schema is not None}\")\n",
    "        # If schema is provided, try to return an empty DF with that schema.\n",
    "        if schema is not None:\n",
    "            try:\n",
    "                return spark_sess.createDataFrame([], schema=schema)\n",
    "            except Exception as schema_e:\n",
    "                logger.exception(f\"Failed even to create empty DataFrame with schema. Error: {schema_e}\")\n",
    "        return None\n",
    "\n",
    "def _create_empty_df_with_schema(spark_sess: SparkSession, full_table_path: str, columns: Optional[List[str]] = None) -> DataFrame:\n",
    "    try:\n",
    "        tbl_schema = spark_sess.read.table(full_table_path).schema\n",
    "        if columns:\n",
    "            requested_lower = {c.lower() for c in columns}\n",
    "            filtered_fields = [f for f in tbl_schema.fields if f.name.lower() in requested_lower]\n",
    "            schema = T.StructType(filtered_fields) if filtered_fields else T.StructType([T.StructField(\"info\", T.StringType(), True)])\n",
    "        else:\n",
    "            schema = tbl_schema\n",
    "        return spark_sess.createDataFrame([], schema=schema)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not create empty DataFrame with schema from '{full_table_path}': {e}. Returning DF with 'info' column.\")\n",
    "        return spark_sess.createDataFrame([], schema=T.StructType([T.StructField(\"info\", T.StringType(), True)]))\n",
    "\n",
    "\n",
    "def parse_to_timestamp_expr(col_name: str, fmt: Optional[str] = None):\n",
    "    \"\"\"Coalesce parse to timestamp using preferred format, default parse, and cast fallback.\"\"\"\n",
    "    if fmt:\n",
    "        return F.coalesce(\n",
    "            F.to_timestamp(F.col(col_name), fmt),\n",
    "            F.to_timestamp(F.col(col_name)),\n",
    "            F.col(col_name).cast(T.TimestampType()),\n",
    "        )\n",
    "    return F.coalesce(\n",
    "        F.to_timestamp(F.col(col_name)),\n",
    "        F.col(col_name).cast(T.TimestampType()),\n",
    "    )\n",
    "\n",
    "\n",
    "def _show_partitions_safe(full_table_path: str, partition_col: Optional[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    List partition values (strings) for a table.\n",
    "    1) SHOW PARTITIONS <table>  -> parse 'col=val' fragments\n",
    "    2) Fallback: DISTINCT(partition_col) if provided (works for views)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = spark.sql(f\"SHOW PARTITIONS `{full_table_path}`\")\n",
    "        vals = []\n",
    "        for row in res.collect():\n",
    "            s = row[0] if row and len(row) > 0 else None\n",
    "            if not s:\n",
    "                continue\n",
    "            last = s.split(\"/\")[-1]\n",
    "            vals.append(last.split(\"=\", 1)[1] if \"=\" in last else last)\n",
    "        return vals\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if partition_col:\n",
    "        try:\n",
    "            df = spark.read.table(full_table_path).select(F.col(partition_col).alias(\"p\")).distinct()\n",
    "            return [str(r[\"p\"]) for r in df.collect() if r and r[\"p\"] is not None]\n",
    "        except Exception:\n",
    "            logger.exception(f\"Fallback DISTINCT failed for {full_table_path}.{partition_col}\")\n",
    "\n",
    "    return []\n",
    "\n",
    "def _month_key(d: date) -> str:\n",
    "    return f\"{d.year:04d}-{d.month:02d}\"\n",
    "\n",
    "def _pick_month_end_partition(partitions: List[str], year: int, month: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    From a list of YYYY-MM-DD (or YYYYMMDD) strings, pick the max 'ods' within that month.\n",
    "    \"\"\"\n",
    "    if not partitions:\n",
    "        return None\n",
    "    wanted_prefix1 = f\"{year:04d}-{month:02d}-\"   # ISO\n",
    "    wanted_prefix2 = f\"{year:04d}{month:02d}\"     # compact\n",
    "    # collect all dates in that month we can parse\n",
    "    candidates: List[Tuple[str, date]] = []\n",
    "    for p in partitions:\n",
    "        d = _parse_ymd_safe(p)\n",
    "        if not d:\n",
    "            continue\n",
    "        if d.year == year and d.month == month:\n",
    "            candidates.append((p, d))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # pick the max date (month-end or last available within that month)\n",
    "    candidates.sort(key=lambda t: t[1])\n",
    "    return candidates[-1][0]\n",
    "\n",
    "def _resolve_daily_retention_days(sys_name: str, sys_conf: Dict[str, Any], tbl_conf: Dict[str, Any]) -> int:\n",
    "    \"\"\"\n",
    "    Priority: table override > system override (conf) > system default map > global default\n",
    "    \"\"\"\n",
    "    if \"daily_retention_days\" in tbl_conf:\n",
    "        return int(tbl_conf[\"daily_retention_days\"])\n",
    "    if \"daily_retention_days\" in (sys_conf or {}):\n",
    "        return int(sys_conf[\"daily_retention_days\"])\n",
    "    if sys_name in DAILY_RETENTION_BY_SYSTEM:\n",
    "        return int(DAILY_RETENTION_BY_SYSTEM[sys_name])\n",
    "    return int(DEFAULT_DAILY_RETENTION_DAYS)\n",
    "\n",
    "def resolve_effective_partition_for_request(\n",
    "    *,\n",
    "    full_table_path: str,\n",
    "    partition_col: str,\n",
    "    requested_date_str: str,\n",
    "    sys_name: str,\n",
    "    sys_conf: Dict[str, Any],\n",
    "    tbl_conf: Dict[str, Any],\n",
    "    today: date\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    If requested_date within retention window => use the requested day (if present).\n",
    "    Otherwise => use month-end (max ods for that month). Generic across systems,\n",
    "    with ICM 45-day default via DAILY_RETENTION_BY_SYSTEM or per-table override.\n",
    "\n",
    "    Returns an ods string to load, or None if nothing found.\n",
    "    \"\"\"\n",
    "    requested_date = _parse_ymd_safe(requested_date_str)\n",
    "    if not requested_date:\n",
    "        logger.warning(\"resolve_effective_partition: requested_date_str not parseable: %s\", requested_date_str)\n",
    "        return None\n",
    "\n",
    "    retention_days = _resolve_daily_retention_days(sys_name, sys_conf, tbl_conf)\n",
    "    cutoff = today - timedelta(days=max(1, retention_days))\n",
    "\n",
    "    # get all known partitions once\n",
    "    parts = _show_partitions_safe(full_table_path, partition_col)\n",
    "\n",
    "    if not parts:\n",
    "        logger.warning(\"No partitions found for %s.\", full_table_path)\n",
    "        return None\n",
    "\n",
    "    # case 1: within daily-window -> prefer exact requested partition if present\n",
    "    if requested_date >= cutoff:\n",
    "        if requested_date_str in parts:\n",
    "            return requested_date_str\n",
    "        # tolerate alternate formats (e.g., YYYYMMDD)\n",
    "        compact = requested_date.strftime(\"%Y%m%d\")\n",
    "        if compact in parts:\n",
    "            return compact\n",
    "        # if exact day not present, fall back to closest earlier day within window\n",
    "        candidates = [(p, _parse_ymd_safe(p)) for p in parts]\n",
    "        candidates = [(p, d) for (p, d) in candidates if d and cutoff <= d <= requested_date]\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda t: t[1], reverse=True)\n",
    "            return candidates[0][0]\n",
    "        return None\n",
    "\n",
    "    # case 2: beyond daily-window -> pick month-end\n",
    "    me = _pick_month_end_partition(parts, requested_date.year, requested_date.month)\n",
    "    if me:\n",
    "        return me\n",
    "\n",
    "    # month has no data => choose latest available before requested_date (month-end of earlier month)\n",
    "    candidates = [(p, _parse_ymd_safe(p)) for p in parts]\n",
    "    candidates = [(p, d) for (p, d) in candidates if d and d <= requested_date]\n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda t: t[1], reverse=True)\n",
    "        return candidates[0][0]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def sample_df(df: DataFrame, n: int, seed: int = 42) -> DataFrame:\n",
    "    \"\"\"Samples up to 'n' rows from a DataFrame. If DataFrame has fewer than n rows, returns all rows.\"\"\"\n",
    "    if df_is_empty(df):\n",
    "        return df\n",
    "    try:\n",
    "        total = df.count() # Materialize count first\n",
    "        if total <= n:\n",
    "            logger.debug(f\"DataFrame has {total} rows, which is <= sample size {n}. Returning all rows.\")\n",
    "            return df\n",
    "        # Calculate fraction for sampling to aim for 'n' rows, add a small buffer\n",
    "        frac = min(1.2 * (n / float(total)), 1.0)\n",
    "        return df.sample(withReplacement=False, fraction=frac, seed=seed).limit(n)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error during DataFrame sampling to get {n} rows.\")\n",
    "        # Fallback: return up to n rows directly if sampling fails.\n",
    "        return df.limit(n)\n",
    "\n",
    "def _safe_float(val: Any) -> Optional[float]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        out = float(val)\n",
    "        if math.isnan(out) or math.isinf(out):\n",
    "            return None\n",
    "        return out\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def _normalize_to_string(val: Any) -> Optional[str]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(val, dict):\n",
    "            return json.dumps(val, ensure_ascii=False, default=str)\n",
    "        return str(val)\n",
    "    except TypeError:\n",
    "        logger.warning(\"normalize_to_string: non-serializable type %s\", type(val).__name__)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _normalize_metric_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a single metric dict to conform to metrics_history_schema:\n",
    "      - Coerce numeric fields\n",
    "      - Uppercase/standardize status\n",
    "      - Normalize threshold/metric_value to text\n",
    "      - Ensure required run metadata defaults\n",
    "    \"\"\"\n",
    "    # relies on a global metrics_history_schema (StructType)\n",
    "    schema_fields = {f.name for f in metrics_history_schema.fields}\n",
    "    out: Dict[str, Any] = {key: None for key in schema_fields}\n",
    "\n",
    "    for key, value in rec.items():\n",
    "        if key not in schema_fields:\n",
    "            continue\n",
    "\n",
    "        if key in (\"metric_value_num\", \"reference_value\"):\n",
    "            out[key] = _safe_float(value)\n",
    "\n",
    "        elif key in (\"metric_value\", \"threshold\", \"details\"):\n",
    "            out[key] = _normalize_to_string(value)\n",
    "\n",
    "        elif key == \"status\":\n",
    "            try:\n",
    "                s = str(value).strip().upper()\n",
    "                valid = {\"PASS\", \"FAIL\", \"UNCOMPUTABLE\", \"WARN\", \"ERROR\"}\n",
    "                if s in valid:\n",
    "                    out[key] = s\n",
    "                elif s in {\"TRUE\", \"1\", \"PASSED\"}:\n",
    "                    out[key] = \"PASS\"\n",
    "                elif s in {\"FALSE\", \"0\", \"FAILED\"}:\n",
    "                    out[key] = \"FAIL\"\n",
    "                else:\n",
    "                    out[key] = \"UNCOMPUTABLE\"\n",
    "            except Exception:\n",
    "                out[key] = \"UNCOMPUTABLE\"\n",
    "\n",
    "        elif key == \"alert_flag\":\n",
    "            # standardize to \"Fail\" or None\n",
    "            if value is None:\n",
    "                out[key] = None\n",
    "            else:\n",
    "                s = str(value).strip().lower()\n",
    "                out[key] = \"Fail\" if s in {\"fail\", \"failed\", \"true\", \"1\", \"alert\"} else None\n",
    "\n",
    "        else:\n",
    "            out[key] = _normalize_to_string(value)\n",
    "\n",
    "    # Fill defaults if missing\n",
    "    if not out.get(\"run_id\"): out[\"run_id\"] = RUN_ID\n",
    "    if not out.get(\"run_ts\"): out[\"run_ts\"] = RUN_TS\n",
    "    if not out.get(\"partition_date\"): out[\"partition_date\"] = CURR_DATE_STR\n",
    "    if not out.get(\"metric_type\"): out[\"metric_type\"] = \"unknown\"\n",
    "    if not out.get(\"status\"): out[\"status\"] = \"PASS\"\n",
    "    if not out.get(\"country\"): out[\"country\"] = REGION\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def sanitize_all_metrics_return_new(all_metrics_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Sanitizes a list of metric records, catching and reporting individual record errors.\"\"\"\n",
    "    sanitized: List[Dict[str, Any]] = []\n",
    "    for i, rec in enumerate(all_metrics_list):\n",
    "        try:\n",
    "            sanitized_rec = _normalize_metric_record(rec)\n",
    "            sanitized.append(sanitized_rec)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to sanitize metric record at index {i}.\")\n",
    "            # Append an error metric for the failed sanitization attempt\n",
    "            sanitized.append({\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": RUN_TS,\n",
    "                \"partition_date\": CURR_DATE_STR,\n",
    "                \"metric_type\": \"metric_sanitization_failure\",\n",
    "                \"metric_value\": f\"Error sanitizing record {_normalize_to_string(rec)}: {e}\",\n",
    "                \"status\": \"UNCOMPUTABLE\",\n",
    "                \"alert_flag\": \"Fail\",\n",
    "                \"country\": REGION\n",
    "            })\n",
    "    return sanitized\n",
    "\n",
    "def parse_buckets_from_metric_value(metric_value: Union[str, Dict[str, Any]]) -> Tuple[Optional[List[float]], Optional[List[int]]]:\n",
    "    try:\n",
    "        obj = json.loads(metric_value) if isinstance(metric_value, str) else metric_value\n",
    "        if not isinstance(obj, dict):\n",
    "            logger.warning(\"parse_buckets: not a dict\")\n",
    "            return None, None\n",
    "        edges, counts = obj.get(\"edges\"), obj.get(\"counts\")\n",
    "        if not isinstance(edges, list) or not isinstance(counts, list):\n",
    "            logger.warning(\"parse_buckets: missing edges/counts\")\n",
    "            return None, None\n",
    "        e2 = [_safe_float(x) for x in edges]\n",
    "        if None in e2:\n",
    "            logger.warning(\"parse_buckets: bad edge value\")\n",
    "            return None, None\n",
    "        c2 = [int(y) for y in counts]\n",
    "        return e2, c2\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(\"parse_buckets: invalid JSON\")\n",
    "        return None, None\n",
    "    except (TypeError, ValueError):\n",
    "        logger.warning(\"parse_buckets: bad element type\")\n",
    "        return None, None\n",
    "    except Exception:\n",
    "        logger.exception(\"parse_buckets: unexpected\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DRIFT CALCULATIONS HELPERS\n",
    "\n",
    "def _counts_to_prob_vector(counts: List[float], eps: float = EPSILON) -> List[float]:\n",
    "    total = float(sum(counts))\n",
    "    if total <= 0:\n",
    "        n = max(1, len(counts))\n",
    "        return [1.0 / n for _ in counts]\n",
    "    return [max(float(c) / total, eps) for c in counts]\n",
    "\n",
    "\n",
    "def compute_chi2_and_df_from_maps(cur_map: Dict[str, int], ref_map: Dict[str, int], eps: float = 1e-9) -> Optional[Tuple[float, int]]:\n",
    "    try:\n",
    "        if not cur_map or not ref_map:\n",
    "            logger.warning(\"Chi2: empty inputs\")\n",
    "            return None\n",
    "        keys = set(cur_map) | set(ref_map)\n",
    "        if not keys:\n",
    "            return 0.0, 0\n",
    "        chi2 = 0.0\n",
    "        nonzero_expected = 0\n",
    "        for k in keys:\n",
    "            o = float(cur_map.get(k, 0.0))\n",
    "            e = float(ref_map.get(k, 0.0))\n",
    "            if e > 0:\n",
    "                nonzero_expected += 1\n",
    "            chi2 += ((o - e) ** 2) / (e + eps)\n",
    "        dof = max(0, nonzero_expected - 1)\n",
    "        return float(chi2), dof\n",
    "    except Exception:\n",
    "        logger.exception(\"Chi2: failure\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _wasserstein_from_buckets(edges: List[float], cur_counts: List[int], ref_counts: List[int]) -> Optional[float]:\n",
    "    try:\n",
    "        if not edges or not cur_counts or not ref_counts:\n",
    "            return None\n",
    "        n_bins = len(edges) - 1\n",
    "        if len(cur_counts) != n_bins or len(ref_counts) != n_bins:\n",
    "            return None\n",
    "        cur_prob = _counts_to_prob_vector(cur_counts)\n",
    "        ref_prob = _counts_to_prob_vector(ref_counts)\n",
    "        cdf_cur = [sum(cur_prob[:i+1]) for i in range(n_bins)]\n",
    "        cdf_ref = [sum(ref_prob[:i+1]) for i in range(n_bins)]\n",
    "        w = 0.0\n",
    "        for i in range(n_bins):\n",
    "            width = float(edges[i+1]) - float(edges[i])\n",
    "            diff = abs(cdf_cur[i] - cdf_ref[i])\n",
    "            w += diff * width\n",
    "        return float(w)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _jensen_shannon_from_maps(cur_map: Dict[str, int], ref_map: Dict[str, int], top_k: Optional[int] = None, eps: float = EPSILON) -> Optional[float]:\n",
    "    try:\n",
    "        cur_map = cur_map or {}\n",
    "        ref_map = ref_map or {}\n",
    "        all_keys = set(cur_map.keys()) | set(ref_map.keys())\n",
    "        if top_k and len(all_keys) > top_k:\n",
    "            combined = {k: cur_map.get(k, 0) + ref_map.get(k, 0) for k in all_keys}\n",
    "            top_keys = set(sorted(combined.keys(), key=lambda k: combined[k], reverse=True)[:top_k])\n",
    "            def reduce_map(m):\n",
    "                out, other = {}, 0\n",
    "                for k, v in m.items():\n",
    "                    if k in top_keys:\n",
    "                        out[k] = v\n",
    "                    else:\n",
    "                        other += v\n",
    "                if other > 0:\n",
    "                    out[\"__OTHER__\"] = other\n",
    "                return out\n",
    "            cur_map = reduce_map(cur_map)\n",
    "            ref_map = reduce_map(ref_map)\n",
    "            all_keys = set(cur_map.keys()) | set(ref_map.keys())\n",
    "        keys = sorted(all_keys)\n",
    "        cur_counts = [float(cur_map.get(k, 0)) for k in keys]\n",
    "        ref_counts = [float(ref_map.get(k, 0)) for k in keys]\n",
    "        p = _counts_to_prob_vector(cur_counts, eps)\n",
    "        q = _counts_to_prob_vector(ref_counts, eps)\n",
    "        m = [(pi + qi) / 2.0 for pi, qi in zip(p, q)]\n",
    "        def kl(a, b):\n",
    "            s = 0.0\n",
    "            for ai, bi in zip(a, b):\n",
    "                if ai <= 0:\n",
    "                    continue\n",
    "                s += ai * math.log(ai / bi)\n",
    "            return s\n",
    "        js = 0.5 * (kl(p, m) + kl(q, m))\n",
    "        return float(js)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _entropy_from_map(m: Dict[str, int], eps: float = EPSILON) -> float:\n",
    "    counts = [float(v) for v in (m or {}).values()]\n",
    "    tot = sum(counts)\n",
    "    if tot <= 0:\n",
    "        return 0.0\n",
    "    probs = [max(c / tot, eps) for c in counts]\n",
    "    return float(-sum(p * math.log(p) for p in probs))\n",
    "\n",
    "def _topk_churn_and_mass_change(cur_map: Dict[str, int], ref_map: Dict[str, int], k: int = 10) -> Tuple[float, int, int, float]:\n",
    "    cur_map = cur_map or {}\n",
    "    ref_map = ref_map or {}\n",
    "    cur_sorted = sorted(cur_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    ref_sorted = sorted(ref_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    topk_cur = [x for x, _ in cur_sorted[:k]]\n",
    "    topk_ref = [x for x, _ in ref_sorted[:k]]\n",
    "    removed = len([x for x in topk_ref if x not in topk_cur])\n",
    "    added = len([x for x in topk_cur if x not in topk_ref])\n",
    "    def topk_share(m, keys):\n",
    "        total = float(sum(m.values()) or 0.0)\n",
    "        if total == 0.0:\n",
    "            return 0.0\n",
    "        return sum(m.get(x, 0) for x in keys) / total\n",
    "    cur_share = topk_share(cur_map, topk_cur)\n",
    "    ref_share = topk_share(ref_map, topk_ref)\n",
    "    mass_delta = cur_share - ref_share\n",
    "    churn = (removed + added) / float(max(1, k))\n",
    "    return float(mass_delta), removed, added, float(churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_region_token(path: str, from_region: str, to_region: str) -> str:\n",
    "    import re\n",
    "    newp = path\n",
    "    patterns = [\n",
    "        (rf\"(?i)([_\\-\\./]){from_region}([_\\-\\./])\", rf\"\\1{to_region}\\2\"),  # _sg_ .sg. -sg-\n",
    "        (rf\"(?i)([_\\-\\./]){from_region}$\", rf\"\\1{to_region}\"),             # _sg end\n",
    "        (rf\"(?i)^{from_region}([_\\-\\./])\", rf\"{to_region}\\1\"),             # sg_ start\n",
    "    ]\n",
    "    for pat, repl in patterns:\n",
    "        newp = re.sub(pat, repl, newp)\n",
    "    return newp\n",
    "\n",
    "def _parse_ymd_safe(s: str) -> Optional[date]:\n",
    "    if not s:\n",
    "        return None\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y%m%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt).date()\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def choose_recent_ref_partition(\n",
    "    partitions: List[str],\n",
    "    anchor_date: date,\n",
    "    max_age_days: int,\n",
    "    *,\n",
    "    require_iso: bool = False\n",
    ") -> Optional[str]:\n",
    "    if not partitions:\n",
    "        logger.warning(\"ref_pick: no partitions\")\n",
    "        return None\n",
    "    cutoff = anchor_date - timedelta(days=max(1, max_age_days))\n",
    "    parsed: List[Tuple[str, date]] = []\n",
    "    for p in partitions:\n",
    "        d = _parse_ymd_safe(p)\n",
    "        if d:\n",
    "            if d <= anchor_date:\n",
    "                parsed.append((p, d))\n",
    "        elif require_iso:\n",
    "            logger.warning(\"ref_pick: reject non-ISO %s\", p)\n",
    "    if parsed:\n",
    "        pool = [t for t in parsed if t[1] >= cutoff] or parsed\n",
    "        pool.sort(key=lambda t: t[1], reverse=True)\n",
    "        return pool[0][0]\n",
    "    if require_iso:\n",
    "        return None\n",
    "    return sorted(partitions)[-1]\n",
    "\n",
    "\n",
    "# ---- partition value formatting (matches storage format/type) -------------------------\n",
    "def _format_partition_literal(\n",
    "    partition_value: str,\n",
    "    pfield_dtype: T.DataType,\n",
    "    pcol: str,\n",
    "    date_columns: Optional[Dict[str, str]]\n",
    ") -> F.Column:\n",
    "    lit_expr = F.lit(partition_value)\n",
    "    fmt = (date_columns or {}).get(pcol)\n",
    "    if isinstance(pfield_dtype, T.DateType):\n",
    "        # e.g., 'yyyy-MM-dd' stored as DATE\n",
    "        return F.to_date(lit_expr) if not fmt or fmt == \"yyyy-MM-dd\" else F.to_date(lit_expr, fmt)\n",
    "    if isinstance(pfield_dtype, T.TimestampType):\n",
    "        return F.to_timestamp(lit_expr) if not fmt or fmt.startswith(\"yyyy-MM-dd\") else F.to_timestamp(lit_expr, fmt)\n",
    "    if isinstance(pfield_dtype, (T.IntegerType, T.LongType, T.ShortType, T.FloatType, T.DoubleType, T.DecimalType, T.BooleanType)):\n",
    "        return lit_expr.cast(pfield_dtype)\n",
    "    if isinstance(pfield_dtype, T.StringType) and fmt:\n",
    "        # Convert \"YYYY-MM-DD\" to target string format (e.g., 'yyyyMMdd' for HOGAN.process_date)\n",
    "        return F.date_format(F.to_date(lit_expr, \"yyyy-MM-dd\"), fmt)\n",
    "    return lit_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psi_from_counts(cur_counts: List[int], ref_counts: List[int], eps: float = 1e-9) -> Optional[float]:\n",
    "    try:\n",
    "        if not cur_counts or not ref_counts or len(cur_counts) != len(ref_counts):\n",
    "            logger.warning(\"PSI: invalid inputs\")\n",
    "            return None\n",
    "        c = [max(eps, float(x)) for x in cur_counts]\n",
    "        r = [max(eps, float(x)) for x in ref_counts]\n",
    "        sc, sr = sum(c), sum(r)\n",
    "        if sc <= 0 or sr <= 0:\n",
    "            logger.warning(\"PSI: zero sums\")\n",
    "            return None\n",
    "        psi = 0.0\n",
    "        for ci, ri in zip(c, r):\n",
    "            pc = ci / sc\n",
    "            pr = ri / sr\n",
    "            psi += (pc - pr) * math.log(pc / pr)\n",
    "        return float(psi)\n",
    "    except Exception:\n",
    "        logger.exception(\"PSI: failure\")\n",
    "        return None\n",
    "\n",
    "def wasserstein_from_buckets(edges: List[float], cur_counts: List[int], ref_counts: List[int], eps: float = 1e-12) -> Optional[float]:\n",
    "    try:\n",
    "        if not edges or not cur_counts or not ref_counts:\n",
    "            logger.warning(\"W1D: missing inputs\")\n",
    "            return None\n",
    "        n = len(edges) - 1\n",
    "        if n <= 0 or len(cur_counts) != n or len(ref_counts) != n:\n",
    "            logger.warning(\"W1D: shape mismatch\")\n",
    "            return None\n",
    "        for i in range(n):\n",
    "            if not (edges[i+1] > edges[i]):\n",
    "                logger.warning(\"W1D: non-monotonic edges at %d\", i)\n",
    "                return None\n",
    "        c = [max(0.0, float(x)) for x in cur_counts]\n",
    "        r = [max(0.0, float(x)) for x in ref_counts]\n",
    "        sc, sr = sum(c), sum(r)\n",
    "        if sc <= 0 or sr <= 0:\n",
    "            logger.warning(\"W1D: zero sums\")\n",
    "            return None\n",
    "        cp = [x / sc for x in c]\n",
    "        rp = [x / sr for x in r]\n",
    "        cdf_c, cdf_r, acc_c, acc_r = [], [], 0.0, 0.0\n",
    "        for i in range(n):\n",
    "            acc_c += cp[i]; cdf_c.append(acc_c)\n",
    "            acc_r += rp[i]; cdf_r.append(acc_r)\n",
    "        w = 0.0\n",
    "        for i in range(n):\n",
    "            w += abs(cdf_c[i] - cdf_r[i]) * max(eps, (edges[i+1] - edges[i]))\n",
    "        return float(w)\n",
    "    except Exception:\n",
    "        logger.exception(\"W1D: failure\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# def _compute_chi2_from_maps(cur_map: Dict[str, int], ref_map: Dict[str, int], eps: float = EPSILON) -> Optional[float]:\n",
    "#     \"\"\"\n",
    "#     Computes Chi-Squared statistic from current and reference category counts.\n",
    "#     This is related to CSI.\n",
    "#     \"\"\"\n",
    "#     if cur_map is None and ref_map is None:\n",
    "#         return None\n",
    "#     cur_map = cur_map or {}\n",
    "#     ref_map = ref_map or {}\n",
    "\n",
    "#     # Get all unique categories present in either map\n",
    "#     all_cats = sorted(set(cur_map.keys()) | set(ref_map.keys()))\n",
    "\n",
    "#     cur_counts = [int(cur_map.get(c, 0)) for c in all_cats]\n",
    "#     ref_counts = [int(ref_map.get(c, 0)) for c in all_cats]\n",
    "\n",
    "#     total_cur = float(sum(cur_counts))\n",
    "#     total_ref = float(sum(ref_counts))\n",
    "#     total_all = total_cur + total_ref\n",
    "\n",
    "#     if total_all == 0:\n",
    "#         return 0.0 # No data, Chi2 is 0.\n",
    "\n",
    "#     chi2 = 0.0\n",
    "#     for i in range(len(all_cats)):\n",
    "#         observed_cur = float(cur_counts[i])\n",
    "#         observed_ref = float(ref_counts[i])\n",
    "\n",
    "#         # Expected counts calculation for independence test\n",
    "#         # Expected_cur = total_cur * (proportion of this category in ref_map or overall)\n",
    "#         # A more standard way is to use marginal totals:\n",
    "#         # Expected_ij = (row_total * column_total) / grand_total\n",
    "#         # Here, row_total = total_cur if current, total_ref if reference\n",
    "#         # column_total = sum of counts for this category across both\n",
    "#         # grand_total = total_all\n",
    "\n",
    "#         # Simplified expectation calculation assuming marginals:\n",
    "#         # E_cur = (total_cur * (observed_cur + observed_ref)) / total_all\n",
    "#         # E_ref = (total_ref * (observed_cur + observed_ref)) / total_all\n",
    "\n",
    "#         # If the dataset has categories only in current or reference, the total for that category across both may be non-zero.\n",
    "#         category_total = observed_cur + observed_ref\n",
    "#         if category_total == 0:\n",
    "#             continue # No counts for this category, skip\n",
    "\n",
    "#         # Expected count for current partition, given marginals\n",
    "#         expected_cur = (total_cur * category_total) / total_all if total_all > 0 else 0.0\n",
    "#         # Expected count for reference partition, given marginals\n",
    "#         expected_ref = (total_ref * category_total) / total_all if total_all > 0 else 0.0\n",
    "\n",
    "#         # Add epsilon to expected values to prevent division by zero\n",
    "#         if expected_cur < eps: expected_cur = eps\n",
    "#         if expected_ref < eps: expected_ref = eps\n",
    "\n",
    "#         try:\n",
    "#             # (Observed - Expected)^2 / Expected\n",
    "#             if observed_cur > 0 or expected_cur > eps: # Only add if there's an observation or a non-negligible expectation\n",
    "#                 chi2 += ((observed_cur - expected_cur) ** 2) / expected_cur\n",
    "#             if observed_ref > 0 or expected_ref > eps: # Only add if there's an observation or a non-negligible expectation\n",
    "#                 chi2 += ((observed_ref - expected_ref) ** 2) / expected_ref\n",
    "#         except ZeroDivisionError:\n",
    "#             logger.warning(f\"Chi2 calculation encountered division by zero for category '{all_cats[i]}'.\")\n",
    "#             continue\n",
    "\n",
    "#     return float(chi2)\n",
    "\n",
    "def compute_numeric_psi_and_buckets(\n",
    "    col: str,\n",
    "    cur_df: DataFrame,\n",
    "    ref_df: Optional[DataFrame],\n",
    "    bins: int = 10,\n",
    "    sample_size: int = 50000,\n",
    ") -> Tuple[Optional[float], Optional[List[float]], Optional[List[int]], Optional[List[int]]]:\n",
    "    \"\"\"\n",
    "    Returns: (psi, edges, cur_counts, ref_counts). Never uses slice keys.\n",
    "    - Edges are numeric list, counts are int list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # prepare numeric column (strip commas, cast)\n",
    "        cur_num = (cur_df\n",
    "                   .select(F.regexp_replace(F.col(col), \",\", \"\").cast(T.DoubleType()).alias(col))\n",
    "                   .where(F.col(col).isNotNull()))\n",
    "        if sample_size and sample_size > 0:\n",
    "            cur_num = cur_num.limit(int(sample_size))\n",
    "\n",
    "        if ref_df is not None:\n",
    "            ref_num = (ref_df\n",
    "                       .select(F.regexp_replace(F.col(col), \",\", \"\").cast(T.DoubleType()).alias(col))\n",
    "                       .where(F.col(col).isNotNull()))\n",
    "            if sample_size and sample_size > 0:\n",
    "                ref_num = ref_num.limit(int(sample_size))\n",
    "        else:\n",
    "            ref_num = None\n",
    "\n",
    "        # choose edges from ref if present, else from current\n",
    "        source_for_edges = ref_num if ref_num is not None else cur_num\n",
    "        if source_for_edges is None or source_for_edges.rdd.isEmpty():\n",
    "            return None, None, None, None\n",
    "\n",
    "        # robust edges via approx quantiles\n",
    "        probs = [i / float(bins) for i in range(bins + 1)]\n",
    "        q = source_for_edges.approxQuantile(col, probs, 0.001)\n",
    "        # ensure strictly increasing and finite\n",
    "        edges = [float(x) for x in q if x is not None and math.isfinite(float(x))]\n",
    "        # if duplicates, spread minimally\n",
    "        if len(edges) >= 2:\n",
    "            dedup = [edges[0]]\n",
    "            for x in edges[1:]:\n",
    "                if x <= dedup[-1]:\n",
    "                    x = dedup[-1] + 1e-9\n",
    "                dedup.append(x)\n",
    "            edges = dedup\n",
    "        if len(edges) < 2:\n",
    "            return None, None, None, None\n",
    "\n",
    "        bucketizer = Bucketizer(splits=edges, inputCol=col, outputCol=\"_dq_bucket\", handleInvalid=\"skip\")\n",
    "\n",
    "        cur_b = (bucketizer.transform(cur_num)\n",
    "                 .groupBy(\"_dq_bucket\").count()\n",
    "                 .withColumnRenamed(\"count\", \"cnt\"))\n",
    "        cur_rows = {int(r[\"_dq_bucket\"]): int(r[\"cnt\"]) for r in cur_b.collect()}\n",
    "\n",
    "        n_bins = len(edges) - 1\n",
    "        cur_counts = [cur_rows.get(i, 0) for i in range(n_bins)]\n",
    "\n",
    "        ref_counts = None\n",
    "        psi = None\n",
    "        if ref_num is not None and not ref_num.rdd.isEmpty():\n",
    "            ref_b = (bucketizer.transform(ref_num)\n",
    "                     .groupBy(\"_dq_bucket\").count()\n",
    "                     .withColumnRenamed(\"count\", \"cnt\"))\n",
    "            ref_rows = {int(r[\"_dq_bucket\"]): int(r[\"cnt\"]) for r in ref_b.collect()}\n",
    "            ref_counts = [ref_rows.get(i, 0) for i in range(n_bins)]\n",
    "            psi = _compute_psi_from_counts(cur_counts, ref_counts, EPSILON)\n",
    "\n",
    "        return psi, edges, cur_counts, ref_counts\n",
    "    except Exception:\n",
    "        logger.exception(\"compute_numeric_psi_and_buckets failed\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def compute_categorical_csi_and_buckets(\n",
    "    col: str,\n",
    "    cur_df: DataFrame,\n",
    "    ref_df: Optional[DataFrame],\n",
    "    sample_size: int = 50000,\n",
    "    max_cardinality: int = 5000,\n",
    ") -> Tuple[Optional[float], Optional[Dict[str, int]], Optional[Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Returns: (chi2_value, cur_map, ref_map)\n",
    "    - cur_map/ref_map: {category (str): count (int)}\n",
    "    - trims to max_cardinality by top frequency to avoid blowups.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        def _counts(df: DataFrame) -> Dict[str, int]:\n",
    "            if df is None: return {}\n",
    "            base = df.select(F.coalesce(F.col(col).cast(T.StringType()), F.lit(\"NULL\")).alias(col))\n",
    "            if sample_size and sample_size > 0:\n",
    "                base = base.limit(int(sample_size))\n",
    "            rows = (base.groupBy(col).count().collect())\n",
    "            m = { (r[col] if r[col] is not None else \"NULL\"): int(r[\"count\"]) for r in rows }\n",
    "            # trim to top-N by freq if too many categories\n",
    "            if len(m) > max_cardinality:\n",
    "                tops = sorted(m.items(), key=lambda kv: kv[1], reverse=True)[:max_cardinality]\n",
    "                m = dict(tops)\n",
    "            return m\n",
    "\n",
    "        cur_map = _counts(cur_df)\n",
    "        ref_map = _counts(ref_df) if ref_df is not None else {}\n",
    "\n",
    "        if not cur_map or not ref_map:\n",
    "            return None, cur_map, ref_map\n",
    "\n",
    "        # align keys\n",
    "        keys = sorted(set(cur_map.keys()) | set(ref_map.keys()))\n",
    "        cur = [float(cur_map.get(k, 0)) for k in keys]\n",
    "        ref = [float(ref_map.get(k, 0)) for k in keys]\n",
    "\n",
    "        # classical chi-square (without Yates); add eps to avoid div-by-zero\n",
    "        eps = EPSILON\n",
    "        chi2 = 0.0\n",
    "        for i in range(len(keys)):\n",
    "            diff = cur[i] - ref[i]\n",
    "            denom = ref[i] + eps\n",
    "            chi2 += (diff * diff) / denom\n",
    "\n",
    "        return float(chi2), cur_map, ref_map\n",
    "    except Exception:\n",
    "        logger.exception(\"compute_categorical_csi_and_buckets failed\")\n",
    "        return None, None, None\n",
    "\n",
    "def _tokenize_expr_for(colname: str) -> str:\n",
    "    # Use the PROTEGRITY_UDF / PROTEGRITY_POLICY strings from config\n",
    "    return f\"{PROTEGRITY_UDF}(trim(`{colname}`),'{PROTEGRITY_POLICY}')\"\n",
    "\n",
    "def tokenize_col_if_needed(df: DataFrame, raw_col_candidates: List[str], token_col_name: str = \"harmonized_key\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Attempt to tokenise first available column in raw_col_candidates.\n",
    "    Always returns a DF that contains token_col_name (null if tokenization not possible).\n",
    "    \"\"\"\n",
    "    resolved = None\n",
    "    for c in raw_col_candidates:\n",
    "        try:\n",
    "            rc = resolve_col_name(df, c)\n",
    "        except Exception:\n",
    "            rc = None\n",
    "        if rc:\n",
    "            resolved = rc\n",
    "            break\n",
    "\n",
    "    existing_token_col = resolve_col_name(df, token_col_name)\n",
    "    if existing_token_col and existing_token_col == token_col_name:\n",
    "        return df\n",
    "\n",
    "    protegrity_available = bool(PROTEGRITY_UDF) and bool(PROTEGRITY_POLICY)\n",
    "    if resolved and protegrity_available:\n",
    "        try:\n",
    "            logger.info(f\"Applying tokenization to column '{resolved}' -> '{token_col_name}' using {PROTEGRITY_UDF}.\")\n",
    "            if existing_token_col:\n",
    "                df = df.drop(existing_token_col)\n",
    "            return df.withColumn(token_col_name, F.expr(_tokenize_expr_for(resolved)))\n",
    "        except Exception:\n",
    "            logger.exception(f\"Protegrity UDF failed for column '{resolved}'. Falling back to null '{token_col_name}'.\")\n",
    "            return df.withColumn(token_col_name, F.lit(None).cast(T.StringType()))\n",
    "    else:\n",
    "        if not resolved:\n",
    "            logger.debug(f\"No candidate column found among {raw_col_candidates} for tokenization.\")\n",
    "        if not protegrity_available:\n",
    "            logger.debug(\"Protegrity not configured; adding null token column.\")\n",
    "        return df.withColumn(token_col_name, F.lit(None).cast(T.StringType()))\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Partitions / IO Helpers\n",
    "def _parse_ymd_safe(s: str) -> Optional[date]:\n",
    "    \"\"\"Best-effort YYYY-MM-DD parser; returns None if parsing fails.\"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(s[:10], \"%Y-%m-%d\").date()\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Load a single partition safely (typed literal; resolved columns)\n",
    "# --------------------------------------------------------------------------------\n",
    "def load_partition_df(\n",
    "    spark_sess: SparkSession,\n",
    "    full_table_path: str,\n",
    "    partition_col: str,\n",
    "    partition_value: str,\n",
    "    columns: Optional[List[str]] = None,\n",
    "    tbl_conf: Optional[Dict[str, Any]] = None,\n",
    ") -> Tuple[Optional[DataFrame], str]:\n",
    "    \"\"\"\n",
    "    Returns (df, status) where status ∈ {'loaded','missing','error'}.\n",
    "    Ensures partition literal matches the storage type/format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_df = spark_sess.read.table(full_table_path)\n",
    "        tbl_schema = base_df.schema\n",
    "        dummy = spark_sess.createDataFrame([], schema=tbl_schema)\n",
    "\n",
    "        pcol_resolved = resolve_col_name(dummy, partition_col)\n",
    "        if not pcol_resolved:\n",
    "            logger.error(f\"Partition column '{partition_col}' not found in '{full_table_path}'.\")\n",
    "            return None, \"error\"\n",
    "\n",
    "        # pick dtype and format partition literal accordingly\n",
    "        pfield = next((f for f in tbl_schema.fields if f.name == pcol_resolved), None)\n",
    "        if pfield is None:\n",
    "            logger.error(f\"Resolved partition column '{pcol_resolved}' missing in schema for '{full_table_path}'.\")\n",
    "            return None, \"error\"\n",
    "\n",
    "        date_fmt_map = (tbl_conf or {}).get(\"date_columns\") if isinstance(tbl_conf, dict) else {}\n",
    "        lit_expr = _format_partition_literal(partition_value, pfield.dataType, pcol_resolved, date_fmt_map)\n",
    "\n",
    "        df = base_df.filter(F.col(pcol_resolved) == lit_expr)\n",
    "\n",
    "        if columns:\n",
    "            keep_cols = resolve_col_list(df, columns)\n",
    "            if pcol_resolved not in keep_cols:\n",
    "                keep_cols.append(pcol_resolved)\n",
    "            keep_cols = list(dict.fromkeys(keep_cols))\n",
    "            if keep_cols:\n",
    "                df = df.select(*[F.col(c) for c in keep_cols])\n",
    "\n",
    "        if df_is_empty(df):\n",
    "            logger.warning(f\"Partition '{pcol_resolved}={partition_value}' in '{full_table_path}' is empty.\")\n",
    "            return _create_empty_df_with_schema(spark_sess, full_table_path, columns), \"missing\"\n",
    "\n",
    "        return df, \"loaded\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error loading {full_table_path} @ {partition_col}={partition_value}: {e}\")\n",
    "        return None, \"error\"\n",
    "\n",
    "\n",
    "\n",
    "def is_numeric_column_by_schema(df: DataFrame, colname: str) -> bool:\n",
    "    \"\"\"Checks if a column in the DataFrame schema is of a numeric type.\"\"\"\n",
    "    if not colname or df is None:\n",
    "        return False\n",
    "    for f in df.schema.fields:\n",
    "        if f.name.lower() == colname.lower():\n",
    "            # Consider standard numeric types\n",
    "            return isinstance(f.dataType, T.NumericType)\n",
    "    return False\n",
    "\n",
    "# ========================\n",
    "# DQMonitor\n",
    "# ========================\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F, types as T\n",
    "\n",
    "class DQMonitor:\n",
    "    \"\"\"\n",
    "    Core DQ monitor:\n",
    "    - Caches loaded partitions and counts (single load per table/partition)\n",
    "    - Emits normalized metrics via _emit_metric\n",
    "    - Uses robust partition discovery and region fallback\n",
    "    - Safe, vectorized checks (row-count, completeness, uniqueness, range, date/latency, join-consistency)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark_session: SparkSession,\n",
    "        table_config: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "        full_table_config: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "        cross_conf: Optional[List[Dict[str, Any]]] = None,\n",
    "        curr_date_obj: Optional[date] = None,\n",
    "        static_thr: Optional[Dict[str, Union[float, int]]] = None,\n",
    "        region: Optional[str] = None,\n",
    "    ):\n",
    "        self.spark = spark_session\n",
    "\n",
    "        supplied_conf = table_config if table_config is not None else full_table_config\n",
    "        if supplied_conf is None:\n",
    "            supplied_conf = get_table_config(region)\n",
    "\n",
    "        self.table_config = validate_and_normalize_table_config(supplied_conf, (region or \"sg\"))\n",
    "        self.full_table_config = validate_and_normalize_table_config(supplied_conf, (region or \"sg\"))\n",
    "\n",
    "        self.cross_conf = cross_conf or []\n",
    "\n",
    "        self.curr_date_obj = curr_date_obj or date.today()\n",
    "        self.curr_date_str = self.curr_date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        self.static_thr = static_thr or {}\n",
    "        self.region = (region or \"sg\").lower()\n",
    "\n",
    "        # Caches\n",
    "        self.all_metrics: List[Dict[str, Any]] = []\n",
    "\n",
    "        # keep cache keys consistent with your current loader signature (sys, tbl, partition)\n",
    "        self.df_cache: Dict[Tuple[str, str, str], Optional[DataFrame]] = {}\n",
    "        self.df_count_cache: Dict[Tuple[str, str, str], int] = {}\n",
    "\n",
    "        self.ref_date_map: Dict[str, Optional[str]] = {}\n",
    "        self.col_stats_cache: Dict[Tuple[str, str, str, str], Dict[str, Optional[float]]] = {}\n",
    "\n",
    "        self.metrics_history_df_cache: Optional[DataFrame] = None\n",
    "        self.metrics_history_loaded = False\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Metric helpers\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _emit_metric(self, rec: Dict[str, Any], partition_date: Optional[str] = None):\n",
    "        \"\"\"Append a normalized metric record (fills run metadata).\"\"\"\n",
    "        base = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"partition_date\": partition_date or self.curr_date_str,\n",
    "            \"run_ts\": RUN_TS,\n",
    "            \"country\": REGION,\n",
    "        }\n",
    "        base.update(rec)\n",
    "        self.all_metrics.append(base)\n",
    "\n",
    "    def _mark_uncomputable(\n",
    "        self,\n",
    "        metric_type: str,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        col_name: Optional[str] = None,\n",
    "        partition_date: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        rec = {\n",
    "            \"metric_type\": metric_type,\n",
    "            \"source_system\": sys,\n",
    "            \"table_name\": tbl,\n",
    "            \"column_name\": col_name,\n",
    "            \"metric_value\": \"uncomputable\",\n",
    "            \"metric_value_num\": None,\n",
    "            \"status\": \"UNCOMPUTABLE\",\n",
    "            \"alert_flag\": \"Fail\",\n",
    "        }\n",
    "        rec.update(kwargs)\n",
    "        self._emit_metric(rec, partition_date=partition_date)\n",
    "\n",
    "    # Instance wrapper so existing code can call self._normalize_metric_record(...)\n",
    "    def _normalize_metric_record(self, rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        return _normalize_metric_record(rec)\n",
    "\n",
    "    def check_cross_system_join_consistency(self, *args, **kwargs):\n",
    "        \"\"\"Back-compat alias -> use key consistency check.\"\"\"\n",
    "        return self.check_cross_system_key_consistency(*args, **kwargs)\n",
    "\n",
    "    def resolve_col_name(self, df: DataFrame, name: Optional[str]) -> Optional[str]:\n",
    "        if not name or df is None:\n",
    "            return None\n",
    "        key = tuple(df.columns)\n",
    "        lm = self._schema_lower_cache.get(key)\n",
    "        if lm is None:\n",
    "            lm = _lower_map(df.columns)\n",
    "            self._schema_lower_cache[key] = lm\n",
    "        return lm.get(name.lower())\n",
    "\n",
    "    def resolve_col_list(self, df: DataFrame, names: List[str]) -> List[str]:\n",
    "        return [r for n in (names or []) if (r := self.resolve_col_name(df, n))]\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Config / partitions\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _get_table_conf(self, sys: str, tbl: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Safe accessor for a (system, table) config; injects system default partition col if missing.\"\"\"\n",
    "        cfg_root = self.table_config or self.full_table_config or {}\n",
    "        sys_conf = (cfg_root.get(sys) or {})\n",
    "        tables = sys_conf.get(\"tables\") or {}\n",
    "        tconf = tables.get(tbl)\n",
    "        if not isinstance(tconf, dict):\n",
    "            return None\n",
    "        if \"default_partition_col\" not in tconf:\n",
    "            tconf = dict(tconf)  # shallow copy\n",
    "            tconf[\"default_partition_col\"] = sys_conf.get(\"default_partition_col\", \"ods\")\n",
    "        return tconf\n",
    "\n",
    "    def _get_ref_partition_date(self, full_table_path: str, partition_col: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Pick a recent reference partition for drift/baselines (cached by full path).\n",
    "        - Prefers ≤ 'today' and within configured max age.\n",
    "        - Falls back to region-swapped path if configured and primary has no partitions.\n",
    "        \"\"\"\n",
    "        if full_table_path in self.ref_date_map:\n",
    "            return self.ref_date_map[full_table_path]\n",
    "\n",
    "        parts = _show_partitions_safe(full_table_path, partition_col)\n",
    "        # Optional region fallback if none found at primary path\n",
    "        if not parts:\n",
    "            try:\n",
    "                current_region = (REGION or \"sg\").lower()\n",
    "                for alt in [r for r in [\"sg\", \"hk\"] if r != current_region]:\n",
    "                    alt_path = _swap_region_token(full_table_path, current_region, alt)\n",
    "                    if alt_path != full_table_path:\n",
    "                        parts = _show_partitions_safe(alt_path, partition_col)\n",
    "                        if parts:\n",
    "                            break\n",
    "            except Exception:\n",
    "                logger.exception(\"Region fallback during ref partition lookup failed.\")\n",
    "\n",
    "        if not parts:\n",
    "            logger.warning(f\"No partitions found for {full_table_path}; reference will be None.\")\n",
    "            self.ref_date_map[full_table_path] = None\n",
    "            return None\n",
    "\n",
    "        max_age = int(self.static_thr.get(\"max_ref_partition_age_days\", MAX_REF_AGE_DAYS))\n",
    "        ref = choose_recent_ref_partition(parts, self.curr_date_obj, max_age)\n",
    "        self.ref_date_map[full_table_path] = ref\n",
    "        return ref\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load & cache a single partition (with region fallback already handled in loader)\n",
    "    # -------------------------------------------------------------------------\n",
    "#     def _load_and_cache_df(\n",
    "#     self,\n",
    "#     sys: str,\n",
    "#     tbl: str,\n",
    "#     partition_date: str,\n",
    "#     tbl_conf: Optional[Dict[str, Any]] = None,\n",
    "# ) -> Optional[DataFrame]:\n",
    "#         \"\"\"\n",
    "#         Load a single partition for (sys, tbl, partition_date) and cache both df and count.\n",
    "\n",
    "#         Guardrail:\n",
    "#           - If source system is EBBS and region is HK, skip loading entirely.\n",
    "#         \"\"\"\n",
    "#         cache_key = (sys, tbl, partition_date)\n",
    "#         if cache_key in self.df_cache:\n",
    "#             return self.df_cache[cache_key]\n",
    "\n",
    "#         # --- Skip EBBS for HK region ---\n",
    "#         current_region = (getattr(self, \"region\", None) or DEFAULT_REGION).strip().lower()\n",
    "#         if sys.upper() == \"EBBS\" and current_region == \"hk\":\n",
    "#             logger.info(f\"Skipping load for {sys}.{tbl}@{partition_date} (region={current_region}).\")\n",
    "#             self.df_cache[cache_key] = None\n",
    "#             self.df_count_cache[cache_key] = 0\n",
    "#             return None\n",
    "\n",
    "#         tbl_conf = tbl_conf or self._get_table_conf(sys, tbl)\n",
    "#         if not tbl_conf:\n",
    "#             logger.error(f\"Table config for {sys}.{tbl} not found.\")\n",
    "#             self.df_cache[cache_key] = None\n",
    "#             self.df_count_cache[cache_key] = 0\n",
    "#             return None\n",
    "\n",
    "#         full_path = tbl_conf.get(\"full_table_path\")\n",
    "#         p_col = _system_partition_col(sys, tbl_conf)\n",
    "#         cols_decl = tbl_conf.get(\"columns\") or []\n",
    "#         cols_to_load = list(cols_decl.keys()) if isinstance(cols_decl, dict) else list(cols_decl)\n",
    "\n",
    "#         df, status = load_partition_df(\n",
    "#             self.spark, full_path, p_col, partition_date, cols_to_load, tbl_conf=tbl_conf\n",
    "#         )\n",
    "\n",
    "#         # Region fallback if empty/missing/error AND a region hint is present\n",
    "#         if (status in (\"error\", \"missing\") or df is None or df_is_empty(df)) and tbl_conf.get(\"region_hint\"):\n",
    "#             try:\n",
    "#                 cur_region = (tbl_conf.get(\"region\") or \"sg\").strip().lower()\n",
    "#                 for alt_region in [r for r in [\"sg\", \"hk\"] if r != cur_region]:\n",
    "#                     attempt_path = _swap_region_token(full_path, cur_region, alt_region)\n",
    "#                     if attempt_path != full_path:\n",
    "#                         logger.info(f\"[{sys}.{tbl}] trying alt region path: {attempt_path}\")\n",
    "#                         df_alt, status_alt = load_partition_df(\n",
    "#                             self.spark, attempt_path, p_col, partition_date, cols_to_load, tbl_conf=tbl_conf\n",
    "#                         )\n",
    "#                         if status_alt == \"loaded\" and df_alt is not None and not df_is_empty(df_alt):\n",
    "#                             df, status, full_path = df_alt, \"loaded\", attempt_path\n",
    "#                             break\n",
    "#             except Exception:\n",
    "#                 logger.exception(\"Region fallback error; continuing with primary load result.\")\n",
    "\n",
    "#         # Cache df + its count\n",
    "#         self.df_cache[cache_key] = df\n",
    "#         self.df_count_cache[cache_key] = 0 if (df is None or df_is_empty(df)) else df.count()\n",
    "\n",
    "#         return df\n",
    "    def _load_and_cache_df(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        partition_date: str,\n",
    "        tbl_conf: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Optional[DataFrame]:\n",
    "        \"\"\"\n",
    "        Load a single partition for (sys,tbl,partition_date) with daily-vs-month-end logic.\n",
    "        Caches df and its count.\n",
    "        \"\"\"\n",
    "        cache_key = (sys, tbl, partition_date)\n",
    "        if cache_key in self.df_cache:\n",
    "            return self.df_cache[cache_key]\n",
    "\n",
    "        tbl_conf = tbl_conf or self._get_table_conf(sys, tbl)\n",
    "        if not tbl_conf:\n",
    "            logger.error(f\"Table config for {sys}.{tbl} not found.\")\n",
    "            self.df_cache[cache_key] = None\n",
    "            self.df_count_cache[cache_key] = 0\n",
    "            return None\n",
    "\n",
    "        full_path = tbl_conf.get(\"full_table_path\")\n",
    "        p_col = _system_partition_col(sys, tbl_conf)\n",
    "        cols_decl = tbl_conf.get(\"columns\") or []\n",
    "        cols_to_load = list(cols_decl.keys()) if isinstance(cols_decl, dict) else list(cols_decl)\n",
    "\n",
    "        # figure effective partition to load (daily vs month-end)\n",
    "        try:\n",
    "            sys_conf = self.table_config.get(sys, {})  # to allow system-level overrides\n",
    "            effective_partition = resolve_effective_partition_for_request(\n",
    "                full_table_path=full_path,\n",
    "                partition_col=p_col,\n",
    "                requested_date_str=partition_date,\n",
    "                sys_name=sys,\n",
    "                sys_conf=sys_conf,\n",
    "                tbl_conf=tbl_conf,\n",
    "                today=self.curr_date_obj  # run anchor\n",
    "            )\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to resolve effective partition for %s.%s @ %s\", sys, tbl, partition_date)\n",
    "            effective_partition = None\n",
    "\n",
    "        if not effective_partition:\n",
    "            logger.warning(\"No effective partition found for %s.%s @ %s\", sys, tbl, partition_date)\n",
    "            self.df_cache[cache_key] = None\n",
    "            self.df_count_cache[cache_key] = 0\n",
    "            return None\n",
    "\n",
    "        # now actually load that partition\n",
    "        df, status = load_partition_df(\n",
    "            self.spark, full_path, p_col, effective_partition, cols_to_load, tbl_conf=tbl_conf\n",
    "        )\n",
    "\n",
    "        # Region fallback if empty/missing/error AND a region hint is present (unchanged)\n",
    "        if (status in (\"error\", \"missing\") or df is None or df_is_empty(df)) and tbl_conf.get(\"region_hint\"):\n",
    "            try:\n",
    "                cur_region = (tbl_conf.get(\"region\") or \"sg\").strip().lower()\n",
    "                for alt_region in [r for r in [\"sg\", \"hk\"] if r != cur_region]:\n",
    "                    attempt_path = _swap_region_token(full_path, cur_region, alt_region)\n",
    "                    if attempt_path != full_path:\n",
    "                        logger.info(f\"[{sys}.{tbl}] trying alt region path: {attempt_path}\")\n",
    "                        df_alt, status_alt = load_partition_df(\n",
    "                            self.spark, attempt_path, p_col, effective_partition, cols_to_load, tbl_conf=tbl_conf\n",
    "                        )\n",
    "                        if status_alt == \"loaded\" and df_alt is not None and not df_is_empty(df_alt):\n",
    "                            df, status, full_path = df_alt, \"loaded\", attempt_path\n",
    "                            break\n",
    "            except Exception:\n",
    "                logger.exception(\"Region fallback error; continuing with primary load result.\")\n",
    "\n",
    "        # Cache df + cached count (you said you don’t need defensive guard, so direct count)\n",
    "        self.df_cache[cache_key] = df\n",
    "        self.df_count_cache[cache_key] = 0 if (df is None or df_is_empty(df)) else df.count()\n",
    "\n",
    "        # Log which partition we ended up using for transparency\n",
    "        logger.info(\"Loaded %s.%s using effective partition %s (requested %s)\", sys, tbl, effective_partition, partition_date)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Column stats (cached)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _get_column_stats(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        partition_date: str,\n",
    "        df: Optional[DataFrame],\n",
    "        col_name: str\n",
    "    ) -> Dict[str, Optional[float]]:\n",
    "        \"\"\"\n",
    "        Basic stats for a column: non_null_count, distinct_count, min, max\n",
    "        (cached per (sys,tbl,partition,col))\n",
    "        \"\"\"\n",
    "        if df is None or df_is_empty(df):\n",
    "            return {\"non_null_count\": 0, \"distinct_count\": 0, \"min\": None, \"max\": None}\n",
    "\n",
    "        resolved_col = resolve_col_name(df, col_name)\n",
    "        if not resolved_col:\n",
    "            logger.warning(f\"Column '{col_name}' not found in DataFrame for stats.\")\n",
    "            return {\"non_null_count\": 0, \"distinct_count\": 0, \"min\": None, \"max\": None}\n",
    "\n",
    "        key = (sys, tbl, partition_date, resolved_col)\n",
    "        if key in self.col_stats_cache:\n",
    "            return self.col_stats_cache[key]\n",
    "\n",
    "        stats_result: Dict[str, Optional[float]] = {\"non_null_count\": None, \"distinct_count\": None, \"min\": None, \"max\": None}\n",
    "        try:\n",
    "            agg_exprs = [\n",
    "                F.sum(F.when(F.col(resolved_col).isNotNull(), 1).otherwise(0)).alias(\"non_null_count\"),\n",
    "                F.countDistinct(F.col(resolved_col)).alias(\"distinct_count\"),\n",
    "                F.min(F.col(resolved_col)).alias(\"minv\"),\n",
    "                F.max(F.col(resolved_col)).alias(\"maxv\"),\n",
    "            ]\n",
    "            agg_result = df.agg(*agg_exprs).first()\n",
    "            if agg_result:\n",
    "                stats_result[\"non_null_count\"] = float(agg_result[\"non_null_count\"] or 0.0)\n",
    "                stats_result[\"distinct_count\"] = float(agg_result[\"distinct_count\"] or 0.0)\n",
    "                stats_result[\"min\"] = _safe_float(agg_result[\"minv\"])\n",
    "                stats_result[\"max\"] = _safe_float(agg_result[\"maxv\"])\n",
    "        except Exception:\n",
    "            logger.exception(f\"Column stats failed for {sys}.{tbl}@{partition_date}.{resolved_col}.\")\n",
    "            stats_result = {\"non_null_count\": None, \"distinct_count\": None, \"min\": None, \"max\": None}\n",
    "\n",
    "        self.col_stats_cache[key] = stats_result\n",
    "        return stats_result\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Row count (current, drift, dynamic)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def compute_rowcount_dynamic_threshold(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        tbl_conf: Dict[str, Any],\n",
    "        lookback_days: int,\n",
    "    ) -> Optional[Dict[str, float]]:\n",
    "        spark_sess = self.spark\n",
    "        full_path = tbl_conf.get(\"full_table_path\")\n",
    "        pcol = _system_partition_col(sys, tbl_conf)\n",
    "        if not full_path or not pcol:\n",
    "            logger.warning(f\"Missing path/partition_col for {sys}.{tbl}\")\n",
    "            return None\n",
    "\n",
    "        parts = _show_partitions_safe(full_path, pcol)\n",
    "        if not parts:\n",
    "            logger.warning(f\"No partitions for {sys}.{tbl}; dynamic threshold unavailable.\")\n",
    "            return None\n",
    "\n",
    "        end_date = CURR_DATE - timedelta(days=1)\n",
    "        start_date = end_date - timedelta(days=max(1, lookback_days) - 1)\n",
    "        window_parts = [p for p in parts if (d := _parse_ymd_safe(p)) and start_date <= d <= end_date]\n",
    "        if not window_parts:\n",
    "            logger.warning(f\"No partitions within lookback window for {sys}.{tbl}.\")\n",
    "            return None\n",
    "\n",
    "        part_vals = \",\".join([f\"'{v}'\" for v in window_parts])\n",
    "        try:\n",
    "            counts_df = spark_sess.sql(\n",
    "                f\"SELECT `{pcol}` as partition_value, COUNT(1) as cnt \"\n",
    "                f\"FROM `{full_path}` WHERE `{pcol}` IN ({part_vals}) GROUP BY `{pcol}`\"\n",
    "            )\n",
    "            rows = [float(r[\"cnt\"]) for r in counts_df.collect()]\n",
    "            if not rows:\n",
    "                return None\n",
    "            n = len(rows)\n",
    "            mean_v = sum(rows) / n\n",
    "            var_v = (sum((x - mean_v) ** 2 for x in rows) / n) if n > 1 else 0.0\n",
    "            std_v = math.sqrt(var_v)\n",
    "            lower = max(0.0, mean_v - 3 * std_v)\n",
    "            upper = mean_v + 3 * std_v\n",
    "            # (Optional) include n_samples if your caller emits it\n",
    "            return {\"mean\": mean_v, \"stddev\": std_v, \"lower\": lower, \"upper\": upper, \"n_samples\": n}\n",
    "        except Exception:\n",
    "            logger.exception(f\"Rowcount dynamic threshold failed for {sys}.{tbl}.\")\n",
    "            return None\n",
    "\n",
    "        dynamic_thresholds = self.compute_rowcount_dynamic_threshold(sys, tbl, tbl_conf, lookback_days=lookback_days)\n",
    "\n",
    "\n",
    "    def check_row_count(\n",
    "        self, sys: str, tbl: str, tbl_conf: Dict[str, Any], cur_df: Optional[DataFrame], partition_date: str\n",
    "    ):\n",
    "        \"\"\"Emit current row count; check drift vs reference; compare to dynamic band.\"\"\"\n",
    "        if cur_df is None:\n",
    "            logger.warning(f\"Current DF is None for {sys}.{tbl}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"row_count\", sys, tbl, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        current_count = self.df_count_cache.get((sys, tbl, partition_date), 0)\n",
    "\n",
    "        # (1) current row count\n",
    "        self._emit_metric({\n",
    "            \"metric_type\": \"row_count\",\n",
    "            \"source_system\": sys,\n",
    "            \"table_name\": tbl,\n",
    "            \"metric_value_num\": float(current_count),\n",
    "            \"metric_value\": str(current_count),\n",
    "            \"status\": \"PASS\",\n",
    "        }, partition_date=partition_date)\n",
    "\n",
    "        # (2) drift vs recent reference\n",
    "        ref_date = self._get_ref_partition_date(tbl_conf.get(\"full_table_path\"), tbl_conf.get(\"partition_col\"))\n",
    "        ref_count = None\n",
    "        if ref_date:\n",
    "            ref_df = self._load_and_cache_df(sys, tbl, ref_date)\n",
    "            if ref_df is not None:\n",
    "                rc = self.df_count_cache.get((sys, tbl, ref_date), 0)\n",
    "                if rc > 0:\n",
    "                    ref_count = rc\n",
    "                else:\n",
    "                    logger.warning(f\"Reference partition {ref_date} for {sys}.{tbl} has zero rows.\")\n",
    "\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"row_count\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"reference_value\": float(ref_count) if ref_count is not None else None,\n",
    "                \"status\": \"PASS\" if ref_count is not None else \"UNCOMPUTABLE\",\n",
    "            }, partition_date=partition_date)\n",
    "\n",
    "            if ref_count:\n",
    "                drift_abs = abs(current_count - ref_count)\n",
    "                drift_pct = drift_abs / ref_count\n",
    "                threshold_pct = float(self.static_thr.get(\"row_count_drift_pct\", 0.01))\n",
    "                status = \"PASS\" if drift_pct <= threshold_pct else \"FAIL\"\n",
    "\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"row_count_drift\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"metric_value_num\": drift_pct,\n",
    "                    \"metric_value\": f\"{drift_pct:.2%}\",\n",
    "                    \"threshold\": f\"<={threshold_pct:.2%}\",\n",
    "                    \"reference_value\": float(ref_count),\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                }, partition_date=partition_date)\n",
    "            else:\n",
    "                self._mark_uncomputable(\"row_count_drift\", sys, tbl, partition_date=partition_date)\n",
    "        else:\n",
    "            logger.warning(f\"No reference partition found for {sys}.{tbl}.\")\n",
    "            self._mark_uncomputable(\"row_count_drift\", sys, tbl, partition_date=partition_date)\n",
    "\n",
    "        # (3) dynamic band (mean±3σ) over history\n",
    "        lookback_days = int(self.static_thr.get(\"historical_partition_window\", 30))\n",
    "        dyn = self.compute_rowcount_dynamic_threshold(sys, tbl, tbl_conf, lookback_days=lookback_days)\n",
    "        if dyn:\n",
    "            lower, upper, mean, std, n = dyn[\"lower\"], dyn[\"upper\"], dyn[\"mean\"], dyn[\"std\"], dyn[\"n_samples\"]\n",
    "            status = \"PASS\" if lower <= current_count <= upper else \"FAIL\"\n",
    "            thr_str = f\"mean={mean:.2f},std={std:.2f},lower={int(lower)},upper={int(upper)},n_samples={int(n)}\"\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"row_count_dynamic_threshold\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"metric_value_num\": float(current_count),\n",
    "                \"metric_value\": f\"{current_count}\",\n",
    "                \"threshold\": thr_str,\n",
    "                \"reference_value\": mean,\n",
    "                \"status\": status,\n",
    "                \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "            }, partition_date=partition_date)\n",
    "        else:\n",
    "            self._mark_uncomputable(\"row_count_dynamic_threshold\", sys, tbl, partition_date=partition_date)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Completeness\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_completeness(\n",
    "        self, sys: str, tbl: str, tbl_conf: Dict[str, Any], cur_df: Optional[DataFrame], partition_date: str\n",
    "    ):\n",
    "        \"\"\"Percent non-null/non-empty per configured columns + overall.\"\"\"\n",
    "        if cur_df is None or df_is_empty(cur_df):\n",
    "            logger.warning(f\"DF None/empty for {sys}.{tbl}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"completeness\", sys, tbl, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        total_rows = self.df_count_cache.get((sys, tbl, partition_date), 0)\n",
    "        if total_rows == 0:\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"completeness\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"metric_value_num\": 100.0,\n",
    "                \"metric_value\": \"100.00%\",\n",
    "                \"status\": \"PASS\",\n",
    "            }, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        cols_to_check = resolve_col_list(cur_df, tbl_conf.get(\"columns\", []))\n",
    "        if not cols_to_check:\n",
    "            logger.warning(f\"No columns configured for completeness in {sys}.{tbl}.\")\n",
    "            return\n",
    "\n",
    "        # Vectorized null/empty counts\n",
    "        agg_exprs = [\n",
    "            F.sum(\n",
    "                F.when(F.col(c).isNull() | (F.trim(F.col(c).cast(T.StringType())) == ''), 1).otherwise(0)\n",
    "            ).alias(f\"nulls__{c}\")\n",
    "            for c in cols_to_check\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            agg_row = cur_df.agg(*agg_exprs).first()\n",
    "        except Exception:\n",
    "            logger.exception(f\"Completeness aggregation failed for {sys}.{tbl}@{partition_date}.\")\n",
    "            for c in cols_to_check:\n",
    "                self._mark_uncomputable(\"completeness\", sys, tbl, c, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        thr = float(self.static_thr.get(\"completeness_pct\", 99.0))\n",
    "        total_nulls = 0\n",
    "\n",
    "        for c in cols_to_check:\n",
    "            null_count = int(agg_row[f\"nulls__{c}\"] or 0)\n",
    "            total_nulls += null_count\n",
    "            pct = (1.0 - (float(null_count) / total_rows)) * 100.0\n",
    "            status = \"PASS\" if pct >= thr else \"FAIL\"\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"completeness\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"column_name\": c,\n",
    "                \"metric_value_num\": pct,\n",
    "                \"metric_value\": f\"{pct:.2f}%\",\n",
    "                \"threshold\": f\">={thr:.2f}%\",\n",
    "                \"status\": status,\n",
    "                \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "            }, partition_date=partition_date)\n",
    "\n",
    "        overall = (1.0 - (float(total_nulls) / (total_rows * len(cols_to_check)))) * 100.0\n",
    "        status_overall = \"PASS\" if overall >= thr else \"FAIL\"\n",
    "        self._emit_metric({\n",
    "            \"metric_type\": \"completeness\",\n",
    "            \"source_system\": sys,\n",
    "            \"table_name\": tbl,\n",
    "            \"column_name\": \"overall\",\n",
    "            \"metric_value_num\": overall,\n",
    "            \"metric_value\": f\"{overall:.2f}%\",\n",
    "            \"threshold\": f\">={thr:.2f}%\",\n",
    "            \"status\": status_overall,\n",
    "            \"alert_flag\": \"Fail\" if status_overall == \"FAIL\" else None,\n",
    "        }, partition_date=partition_date)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Uniqueness\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_uniqueness(\n",
    "        self, sys: str, tbl: str, tbl_conf: Dict[str, Any], cur_df: Optional[DataFrame], partition_date: str\n",
    "    ):\n",
    "        \"\"\"Percent of distinct key-combinations for the configured join_key(s).\"\"\"\n",
    "        if cur_df is None or df_is_empty(cur_df):\n",
    "            logger.warning(f\"DF None/empty for {sys}.{tbl}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"uniqueness\", sys, tbl, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        total = self.df_count_cache.get((sys, tbl, partition_date), 0)\n",
    "        if total == 0:\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"uniqueness\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"metric_value_num\": 100.0,\n",
    "                \"metric_value\": \"100.00%\",\n",
    "                \"status\": \"PASS\",\n",
    "            }, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        join_key_config = tbl_conf.get(\"join_key\")\n",
    "        if not join_key_config:\n",
    "            logger.warning(f\"No 'join_key' specified in {sys}.{tbl}.\")\n",
    "            return\n",
    "\n",
    "        if isinstance(join_key_config, list):\n",
    "            resolved_keys = resolve_col_list(cur_df, join_key_config)\n",
    "        else:\n",
    "            rk = resolve_col_name(cur_df, join_key_config)\n",
    "            resolved_keys = [rk] if rk else []\n",
    "\n",
    "        if not resolved_keys:\n",
    "            logger.error(f\"None of keys {join_key_config} found in {sys}.{tbl}.\")\n",
    "            self._mark_uncomputable(\"uniqueness\", sys, tbl, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            distinct_count = cur_df.select(*[F.col(k) for k in resolved_keys]).distinct().count()\n",
    "            pct = (float(distinct_count) / total) * 100.0 if total > 0 else 100.0\n",
    "            thr = float(self.static_thr.get(\"uniqueness_pct\", 99.9))\n",
    "            status = \"PASS\" if pct >= thr else \"FAIL\"\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"uniqueness\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"column_name\": \",\".join(resolved_keys),\n",
    "                \"metric_value_num\": pct,\n",
    "                \"metric_value\": f\"{pct:.2f}%\",\n",
    "                \"threshold\": f\">={thr:.2f}%\",\n",
    "                \"status\": status,\n",
    "                \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "            }, partition_date=partition_date)\n",
    "        except Exception:\n",
    "            logger.exception(f\"Uniqueness check failed for {sys}.{tbl}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"uniqueness\", sys, tbl, \",\".join(resolved_keys), partition_date=partition_date)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Range check (vectorized)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_range(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        tbl_conf: Dict[str, Any],\n",
    "        cur_df: Optional[DataFrame],\n",
    "        partition_date: str\n",
    "    ):\n",
    "        \"\"\"Percent of values out-of-range for each configured numeric column.\"\"\"\n",
    "        if cur_df is None or df_is_empty(cur_df):\n",
    "            logger.warning(f\"DF None/empty for {sys}.{tbl}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"range_check\", sys, tbl, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        total = self.df_count_cache.get((sys, tbl, partition_date))\n",
    "        if total is None:\n",
    "            try:\n",
    "                total = cur_df.count()\n",
    "            except Exception:\n",
    "                logger.exception(\"Counting DF failed for range check; default total=0.\")\n",
    "                total = 0\n",
    "            finally:\n",
    "                self.df_count_cache[(sys, tbl, partition_date)] = total\n",
    "\n",
    "        if total == 0:\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"range_check\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"metric_value_num\": 0.0,\n",
    "                \"metric_value\": \"0.00%\",\n",
    "                \"status\": \"PASS\",\n",
    "            }, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        ranges: Dict[str, Dict[str, Any]] = (tbl_conf.get(\"numerical_ranges\") or {})\n",
    "        if not ranges:\n",
    "            logger.info(f\"No numerical ranges for {sys}.{tbl}.\")\n",
    "            return\n",
    "\n",
    "        resolved_items: List[Tuple[str, str, float, float]] = []\n",
    "        for colname, bounds in ranges.items():\n",
    "            resolved = resolve_col_name(cur_df, colname)\n",
    "            if not resolved:\n",
    "                logger.warning(f\"Range col '{colname}' not found in {sys}.{tbl}.\")\n",
    "                self._mark_uncomputable(\"range_check\", sys, tbl, colname, partition_date=partition_date)\n",
    "                continue\n",
    "            try:\n",
    "                mn = float(bounds.get(\"min\", float(\"-inf\")))\n",
    "                mx = float(bounds.get(\"max\", float(\"inf\")))\n",
    "            except Exception:\n",
    "                logger.exception(f\"Invalid bounds for {sys}.{tbl}.{colname}.\")\n",
    "                self._mark_uncomputable(\"range_check\", sys, tbl, colname, partition_date=partition_date)\n",
    "                continue\n",
    "            resolved_items.append((colname, resolved, mn, mx))\n",
    "\n",
    "        if not resolved_items:\n",
    "            return\n",
    "\n",
    "        agg_exprs, out_keys = [], []\n",
    "        for orig_col, resolved_col, mn, mx in resolved_items:\n",
    "            casted = F.when(F.col(resolved_col).isNotNull(), F.col(resolved_col).cast(T.DoubleType())).otherwise(F.lit(None))\n",
    "            oor_flag = F.when(F.col(resolved_col).isNotNull() & casted.isNull(), 1) \\\n",
    "                        .when(casted.isNotNull() & ((casted < F.lit(mn)) | (casted > F.lit(mx))), 1) \\\n",
    "                        .otherwise(0)\n",
    "            alias_name = f\"__oor__{resolved_col}\"\n",
    "            agg_exprs.append(F.sum(oor_flag).alias(alias_name))\n",
    "            out_keys.append((orig_col, alias_name, mn, mx))\n",
    "\n",
    "        try:\n",
    "            agg_row = cur_df.agg(*agg_exprs).first()\n",
    "        except Exception:\n",
    "            logger.exception(f\"Range aggregation failed for {sys}.{tbl}@{partition_date}.\")\n",
    "            for orig_col, _, _, _ in out_keys:\n",
    "                self._mark_uncomputable(\"range_check\", sys, tbl, orig_col, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        max_allowed_pct = float(self.static_thr.get(\"max_out_of_range_pct\", 0.0))\n",
    "        for orig_col, alias_name, mn, mx in out_keys:\n",
    "            try:\n",
    "                oor = float(agg_row[alias_name] or 0.0)\n",
    "                pct = (oor / float(total)) * 100.0 if total > 0 else 0.0\n",
    "                status = \"PASS\" if pct <= max_allowed_pct else \"FAIL\"\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"range_check\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": orig_col,\n",
    "                    \"metric_value_num\": pct,\n",
    "                    \"metric_value\": f\"{pct:.2f}%\",\n",
    "                    \"threshold\": f\"max_out_of_range_pct<={max_allowed_pct:.2f}%\",\n",
    "                    \"status\": status,\n",
    "                    \"details\": f\"[{mn}, {mx}]\",\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                }, partition_date=partition_date)\n",
    "            except Exception:\n",
    "                logger.exception(f\"Emit range metric failed for {sys}.{tbl}.{orig_col}@{partition_date}.\")\n",
    "                self._mark_uncomputable(\"range_check\", sys, tbl, orig_col, partition_date=partition_date)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Date parsing validity + Latency\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_date_logic(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        tbl_conf: Dict[str, Any],\n",
    "        cur_df: Optional[DataFrame],\n",
    "        partition_date: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        (1) Date-format validity for configured date_columns (by format)\n",
    "        (2) Data latency (max timestamp/date vs current run date)\n",
    "        \"\"\"\n",
    "        if cur_df is None or df_is_empty(cur_df):\n",
    "            logger.warning(f\"DF None/empty for {sys}.{tbl}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"date_format_validity\", sys, tbl, partition_date=partition_date)\n",
    "            self._mark_uncomputable(\"data_latency\", sys, tbl, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        total = self.df_count_cache.get((sys, tbl, partition_date), 0)\n",
    "        if total == 0:\n",
    "            self._emit_metric({\"metric_type\": \"date_format_validity\", \"source_system\": sys, \"table_name\": tbl,\n",
    "                               \"metric_value_num\": 100.0, \"metric_value\": \"100.00%\", \"status\": \"PASS\"}, partition_date)\n",
    "            self._emit_metric({\"metric_type\": \"data_latency\", \"source_system\": sys, \"table_name\": tbl,\n",
    "                               \"metric_value_num\": 0.0, \"metric_value\": \"0 days\", \"status\": \"PASS\"}, partition_date)\n",
    "            return\n",
    "\n",
    "        # (1) Date-format validity\n",
    "        date_cols_config: Dict[str, str] = (tbl_conf.get(\"date_columns\") or {})\n",
    "        if date_cols_config:\n",
    "            thr = float(self.static_thr.get(\"date_parse_min_pct\", 95.0))\n",
    "            for dcol, fmt in date_cols_config.items():\n",
    "                resolved = resolve_col_name(cur_df, dcol)\n",
    "                if not resolved:\n",
    "                    logger.warning(f\"Date column '{dcol}' not found in {sys}.{tbl}.\")\n",
    "                    self._mark_uncomputable(\"date_format_validity\", sys, tbl, dcol, partition_date=partition_date)\n",
    "                    continue\n",
    "                try:\n",
    "                    # Count parsable rows directly (no temp cols)\n",
    "                    parsed = parse_to_timestamp_expr(resolved, fmt)\n",
    "                    valid_cnt = cur_df.select(F.sum(F.when(parsed.isNotNull(), 1).otherwise(0)).alias(\"v\")).first()[\"v\"] or 0\n",
    "                    pct_valid = (float(valid_cnt) / float(total)) * 100.0\n",
    "                    status = \"PASS\" if pct_valid >= thr else \"FAIL\"\n",
    "                    self._emit_metric({\n",
    "                        \"metric_type\": \"date_format_validity\",\n",
    "                        \"source_system\": sys,\n",
    "                        \"table_name\": tbl,\n",
    "                        \"column_name\": dcol,\n",
    "                        \"metric_value_num\": pct_valid,\n",
    "                        \"metric_value\": f\"{pct_valid:.2f}%\",\n",
    "                        \"threshold\": f\">={thr:.2f}%\",\n",
    "                        \"status\": status,\n",
    "                        \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    }, partition_date=partition_date)\n",
    "                except Exception:\n",
    "                    logger.exception(f\"Date-format validity failed for {sys}.{tbl}.{dcol}@{partition_date}.\")\n",
    "                    self._mark_uncomputable(\"date_format_validity\", sys, tbl, dcol, partition_date=partition_date)\n",
    "        else:\n",
    "            logger.info(f\"No date columns configured for {sys}.{tbl}; skipping date format validity.\")\n",
    "\n",
    "        # (2) Data latency from timestamp_col\n",
    "        ts_col_config = tbl_conf.get(\"timestamp_col\")\n",
    "        if not ts_col_config:\n",
    "            logger.info(f\"No timestamp column configured for latency in {sys}.{tbl}.\")\n",
    "            return\n",
    "\n",
    "        resolved_ts = resolve_col_name(cur_df, ts_col_config)\n",
    "        if not resolved_ts:\n",
    "            logger.warning(f\"Timestamp column '{ts_col_config}' not found in {sys}.{tbl}.\")\n",
    "            self._mark_uncomputable(\"data_latency\", sys, tbl, ts_col_config, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # If already Date/Timestamp, max works; else parse best-effort\n",
    "            dtype_map = {f.name: f.dataType for f in cur_df.schema.fields}\n",
    "            dtype = dtype_map.get(resolved_ts)\n",
    "            ts_col = F.col(resolved_ts)\n",
    "            max_col = F.max(ts_col) if isinstance(dtype, (T.TimestampType, T.DateType)) else F.max(F.to_timestamp(ts_col))\n",
    "            max_ts = cur_df.select(max_col.alias(\"maxv\")).first()[\"maxv\"]\n",
    "\n",
    "            if max_ts is None:\n",
    "                self._mark_uncomputable(\"data_latency\", sys, tbl, resolved_ts, partition_date=partition_date)\n",
    "                return\n",
    "\n",
    "            if isinstance(max_ts, datetime):\n",
    "                age_days = (self.curr_date_obj - max_ts.date()).days\n",
    "            elif isinstance(max_ts, date):\n",
    "                age_days = (self.curr_date_obj - max_ts).days\n",
    "            else:\n",
    "                try:\n",
    "                    parsed_dt = datetime.fromisoformat(str(max_ts).replace(\"Z\", \"+00:00\"))\n",
    "                    age_days = (self.curr_date_obj - parsed_dt.date()).days\n",
    "                except Exception:\n",
    "                    self._mark_uncomputable(\"data_latency\", sys, tbl, resolved_ts, partition_date=partition_date)\n",
    "                    return\n",
    "\n",
    "            max_age_threshold = int(self.static_thr.get(\"latency_days\", 1))\n",
    "            status = \"PASS\" if age_days <= max_age_threshold else \"FAIL\"\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"data_latency\",\n",
    "                \"source_system\": sys,\n",
    "                \"table_name\": tbl,\n",
    "                \"column_name\": resolved_ts,\n",
    "                \"metric_value_num\": float(age_days),\n",
    "                \"metric_value\": f\"{age_days} days\",\n",
    "                \"threshold\": f\"<={max_age_threshold} days\",\n",
    "                \"status\": status,\n",
    "                \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "            }, partition_date=partition_date)\n",
    "        except Exception:\n",
    "            logger.exception(f\"Data latency failed for {sys}.{tbl}.{resolved_ts}@{partition_date}.\")\n",
    "            self._mark_uncomputable(\"data_latency\", sys, tbl, resolved_ts, partition_date=partition_date)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Join consistency\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_join_consistency(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        tbl_conf: Dict[str, Any],\n",
    "        cur_df: Optional[DataFrame],\n",
    "        partition_date: str,\n",
    "        table_run_cache: Dict[Tuple[str, str], Optional[DataFrame]],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For each configured join, measure % of DISTINCT left keys missing in the right table (same partition_date).\n",
    "        Threshold interpreted as required consistency pct; we compute allowed missing pct = 100 - threshold.\n",
    "        \"\"\"\n",
    "        joins = tbl_conf.get(\"joins\") or []\n",
    "        if not joins:\n",
    "            return\n",
    "\n",
    "        for j in joins:\n",
    "            other_sys = j.get(\"other_system\")\n",
    "            other_tbl = j.get(\"other_table\")\n",
    "            left_key_config = j.get(\"left_key\")\n",
    "            right_key_config = j.get(\"right_key\")\n",
    "\n",
    "            if not all([other_sys, other_tbl, left_key_config, right_key_config]):\n",
    "                logger.warning(f\"Incomplete join configuration for {sys}.{tbl}: {j}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            if cur_df is None or df_is_empty(cur_df):\n",
    "                logger.warning(f\"DF None/empty for {sys}.{tbl}@{partition_date}.\")\n",
    "                self._mark_uncomputable(\"join_consistency\", sys, tbl, left_key_config, partition_date=partition_date)\n",
    "                continue\n",
    "\n",
    "            # Other DF from cache or on-demand\n",
    "            other_df = table_run_cache.get((other_sys, other_tbl))\n",
    "            if other_df is None:\n",
    "                other_conf = self._get_table_conf(other_sys, other_tbl)\n",
    "                if other_conf:\n",
    "                    other_df = self._load_and_cache_df(other_sys, other_tbl, partition_date, tbl_conf=other_conf)\n",
    "                    table_run_cache[(other_sys, other_tbl)] = other_df\n",
    "                else:\n",
    "                    logger.error(f\"Config for join target '{other_sys}.{other_tbl}' not found.\")\n",
    "                    self._mark_uncomputable(\"join_consistency\", sys, tbl, left_key_config, partition_date=partition_date)\n",
    "                    continue\n",
    "\n",
    "            if other_df is None or df_is_empty(other_df):\n",
    "                logger.warning(f\"Target DF '{other_sys}.{other_tbl}'@{partition_date} missing/empty.\")\n",
    "                self._mark_uncomputable(\"join_consistency\", sys, tbl, left_key_config, partition_date=partition_date)\n",
    "                continue\n",
    "\n",
    "            lk_resolved = resolve_col_name(cur_df, left_key_config)\n",
    "            rk_resolved = resolve_col_name(other_df, right_key_config)\n",
    "            if not lk_resolved or not rk_resolved:\n",
    "                logger.error(f\"Join key resolution failed: {sys}.{tbl}.{left_key_config} or {other_sys}.{other_tbl}.{right_key_config}.\")\n",
    "                self._mark_uncomputable(\"join_consistency\", sys, tbl, left_key_config, partition_date=partition_date)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                total_left = self.df_count_cache.get((sys, tbl, partition_date), 0)\n",
    "                if total_left == 0:\n",
    "                    self._emit_metric({\n",
    "                        \"metric_type\": \"join_consistency\",\n",
    "                        \"source_system\": sys,\n",
    "                        \"table_name\": tbl,\n",
    "                        \"column_name\": left_key_config,\n",
    "                        \"metric_value_num\": 0.0,\n",
    "                        \"metric_value\": \"0.00%\",\n",
    "                        \"status\": \"PASS\",\n",
    "                    }, partition_date=partition_date)\n",
    "                    continue\n",
    "\n",
    "                # Distinct keys\n",
    "                right_keys_df = other_df.select(F.col(rk_resolved).alias(rk_resolved)).distinct()\n",
    "                left_keys_df  =  cur_df.select(F.col(lk_resolved).alias(lk_resolved)).distinct()\n",
    "\n",
    "                # Left-anti: keys present on left but missing on right\n",
    "                missing_keys_df = left_keys_df.join(\n",
    "                    right_keys_df,\n",
    "                    left_keys_df[lk_resolved] == right_keys_df[rk_resolved],\n",
    "                    \"left_anti\",\n",
    "                )\n",
    "                missing_count = missing_keys_df.count()\n",
    "                distinct_left_key_count = left_keys_df.count()\n",
    "                pct_missing = (float(missing_count) / distinct_left_key_count) * 100.0 if distinct_left_key_count > 0 else 0.0\n",
    "\n",
    "                req_consistency = float(self.static_thr.get(\"join_consistency_pct\", 95.0))\n",
    "                allowed_missing_pct = 100.0 - req_consistency\n",
    "                status = \"PASS\" if pct_missing <= allowed_missing_pct else \"FAIL\"\n",
    "\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"join_consistency\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": left_key_config,\n",
    "                    \"metric_value_num\": pct_missing,\n",
    "                    \"metric_value\": f\"{pct_missing:.2f}%\",\n",
    "                    \"threshold\": f\"allowed_missing_pct<={allowed_missing_pct:.2f}% (from {req_consistency:.2f}% consistency)\",\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    \"reference_value\": float(distinct_left_key_count),\n",
    "                }, partition_date=partition_date)\n",
    "            except Exception:\n",
    "                logger.exception(\n",
    "                    f\"Join consistency failed for {sys}.{tbl} ({left_key_config}) vs {other_sys}.{other_tbl} ({right_key_config})@{partition_date}.\"\n",
    "                )\n",
    "                self._mark_uncomputable(\"join_consistency\", sys, tbl, left_key_config, partition_date=partition_date)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "    # Cross-system key consistency\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_cross_system_key_consistency(\n",
    "        self,\n",
    "        mapping: Dict[str, Any],\n",
    "        table_run_cache: Dict[Tuple[str, str], Optional[DataFrame]],\n",
    "        partition_date: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ensure keys in (sf.st.sk) exist in (tf.tt.tk). Optionally tokenizes both sides.\n",
    "        - Consistency threshold = static_thr['cross_system_key_consistency_pct'] (default 90%)\n",
    "        - We compute pct_missing on DISTINCT source keys using left-anti join.\n",
    "        \"\"\"\n",
    "        sf = mapping.get(\"source_system_from\")\n",
    "        st = mapping.get(\"table_from\")\n",
    "        sk_config = mapping.get(\"key_from\")\n",
    "\n",
    "        tf = mapping.get(\"source_system_to\")\n",
    "        tt = mapping.get(\"table_to\")\n",
    "        tk_config = mapping.get(\"key_to\")\n",
    "\n",
    "        tokenize = bool(mapping.get(\"tokenize\", False))\n",
    "\n",
    "        if not all([sf, st, sk_config, tf, tt, tk_config]):\n",
    "            logger.warning(f\"Incomplete cross-system mapping config: {mapping}. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # Load/cached A\n",
    "        df_a = table_run_cache.get((sf, st))\n",
    "        if df_a is None:\n",
    "            conf_a = self._get_table_conf(sf, st)\n",
    "            if conf_a:\n",
    "                df_a = self._load_and_cache_df(sf, st, partition_date, tbl_conf=conf_a)\n",
    "                table_run_cache[(sf, st)] = df_a\n",
    "            else:\n",
    "                logger.error(f\"Config for source table '{sf}.{st}' not found.\")\n",
    "                self._mark_uncomputable(\"cross_system_consistency\", sf, st, sk_config, partition_date=partition_date)\n",
    "                return\n",
    "\n",
    "        # Load/cached B\n",
    "        df_b = table_run_cache.get((tf, tt))\n",
    "        if df_b is None:\n",
    "            conf_b = self._get_table_conf(tf, tt)\n",
    "            if conf_b:\n",
    "                df_b = self._load_and_cache_df(tf, tt, partition_date, tbl_conf=conf_b)\n",
    "                table_run_cache[(tf, tt)] = df_b\n",
    "            else:\n",
    "                logger.error(f\"Config for target table '{tf}.{tt}' not found.\")\n",
    "                self._mark_uncomputable(\"cross_system_consistency\", sf, st, sk_config, partition_date=partition_date)\n",
    "                return\n",
    "\n",
    "        if df_a is None or df_is_empty(df_a) or df_b is None or df_is_empty(df_b):\n",
    "            logger.warning(f\"One/both DFs missing/empty for cross-system check: {sf}.{st}, {tf}.{tt} @ {partition_date}.\")\n",
    "            self._mark_uncomputable(\"cross_system_consistency\", sf, st, sk_config, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        # Resolve key columns\n",
    "        resolved_sk = resolve_col_name(df_a, sk_config)\n",
    "        resolved_tk = resolve_col_name(df_b, tk_config)\n",
    "        if not resolved_sk or not resolved_tk:\n",
    "            logger.error(f\"Key resolution failed: '{sk_config}' in {sf}.{st} or '{tk_config}' in {tf}.{tt}.\")\n",
    "            self._mark_uncomputable(\"cross_system_consistency\", sf, st, sk_config, partition_date=partition_date)\n",
    "            return\n",
    "\n",
    "        # Optional tokenization (produces _harm_key_a / _harm_key_b)\n",
    "        if tokenize:\n",
    "            df_a = tokenize_col_if_needed(df_a, [resolved_sk], token_col_name=\"_harm_key_a\")\n",
    "            df_b = tokenize_col_if_needed(df_b, [resolved_tk], token_col_name=\"_harm_key_b\")\n",
    "            lk_resolved, rk_resolved = \"_harm_key_a\", \"_harm_key_b\"\n",
    "            if lk_resolved not in df_a.columns or rk_resolved not in df_b.columns:\n",
    "                logger.error(\"Tokenization failed to produce harmonized key columns.\")\n",
    "                self._mark_uncomputable(\"cross_system_consistency\", sf, st, sk_config, partition_date=partition_date)\n",
    "                return\n",
    "        else:\n",
    "            lk_resolved, rk_resolved = resolved_sk, resolved_tk\n",
    "\n",
    "        try:\n",
    "            src_keys = df_a.select(F.col(lk_resolved).alias(lk_resolved)).distinct()\n",
    "            total_src = src_keys.count()\n",
    "            if total_src == 0:\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"cross_system_consistency\",\n",
    "                    \"source_system\": sf,\n",
    "                    \"table_name\": st,\n",
    "                    \"column_name\": sk_config,\n",
    "                    \"metric_value_num\": 0.0,\n",
    "                    \"metric_value\": \"0.00%\",\n",
    "                    \"status\": \"PASS\",\n",
    "                }, partition_date=partition_date)\n",
    "                return\n",
    "\n",
    "            tgt_keys = df_b.select(F.col(rk_resolved).alias(rk_resolved)).distinct()\n",
    "\n",
    "            # left_anti: keys present in source but missing in target\n",
    "            missing = src_keys.join(tgt_keys, src_keys[lk_resolved] == tgt_keys[rk_resolved], \"left_anti\")\n",
    "            miss_cnt = missing.count()\n",
    "            pct_missing = (float(miss_cnt) / float(total_src)) * 100.0\n",
    "\n",
    "            required_pct = float(self.static_thr.get(\"cross_system_key_consistency_pct\", 90.0))\n",
    "            allowed_missing = 100.0 - required_pct\n",
    "            status = \"PASS\" if pct_missing <= allowed_missing else \"FAIL\"\n",
    "\n",
    "            self._emit_metric({\n",
    "                \"metric_type\": \"cross_system_consistency\",\n",
    "                \"source_system\": sf,\n",
    "                \"table_name\": st,\n",
    "                \"column_name\": sk_config,\n",
    "                \"metric_value_num\": pct_missing,\n",
    "                \"metric_value\": f\"{pct_missing:.2f}%\",\n",
    "                \"threshold\": f\"allowed_missing_pct<={allowed_missing:.2f}% (from {required_pct:.2f}% consistency)\",\n",
    "                \"status\": status,\n",
    "                \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                \"reference_value\": float(total_src),\n",
    "            }, partition_date=partition_date)\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(f\"Cross-system key consistency failed for mapping {mapping} @ {partition_date}.\")\n",
    "            self._mark_uncomputable(\"cross_system_consistency\", sf, st, sk_config, partition_date=partition_date)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Distribution drift (numeric+categorical) — PSI/CSI + Wazirstrian + Genshin Shannon\n",
    "    # -------------------------------------------------------------------------\n",
    "    def check_distribution_drift(\n",
    "        self,\n",
    "        sys: str,\n",
    "        tbl: str,\n",
    "        tbl_conf: Dict[str, Any],\n",
    "        cur_df: Optional[DataFrame],\n",
    "        partition_date: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute distribution drift for columns configured as:\n",
    "          - numeric_drift_cols: PSI, Wazirstrian (Wasserstein), and Genshin Shannon ratio\n",
    "          - categorical_drift_cols (or inferred from 'columns'): CSI (+ Genshin Shannon on category pmfs)\n",
    "        Uses helper fns that NEVER create slice-keys, so no 'unhashable type: slice' errors.\n",
    "        \"\"\"\n",
    "        if cur_df is None or df_is_empty(cur_df):\n",
    "            logger.warning(f\"DF None/empty for {sys}.{tbl}@{partition_date}. Skipping distribution drift.\")\n",
    "            return\n",
    "\n",
    "        # Reference partition (recent, by config)\n",
    "        ref_date = self._get_ref_partition_date(tbl_conf.get(\"full_table_path\"), tbl_conf.get(\"partition_col\"))\n",
    "        ref_df = None\n",
    "        if ref_date:\n",
    "            ref_df = self._load_and_cache_df(sys, tbl, ref_date, tbl_conf=tbl_conf)\n",
    "            if ref_df is None or df_is_empty(ref_df):\n",
    "                logger.warning(f\"Reference DF empty for {sys}.{tbl}@{ref_date}. Proceeding with PSI=None.\")\n",
    "                ref_df = None\n",
    "\n",
    "        # Thresholds\n",
    "        psi_max = float(self.static_thr.get(\"numeric_psi_max\", 0.20))\n",
    "        csi_max = float(self.static_thr.get(\"categorical_csi_max\", 50.0))\n",
    "        waz_max = float(self.static_thr.get(\"wazirstrian_wasserstein_max\", 1.0))  # your custom threshold\n",
    "        genshin_max = float(self.static_thr.get(\"genshin_shannon_max\", 1.15))      # your custom threshold (ratio cap)\n",
    "\n",
    "        bins = int(self.static_thr.get(\"drift_bins\", DRIFT_BINS))\n",
    "        sample_size = int(self.static_thr.get(\"drift_sample_size\", DRIFT_SAMPLE_SIZE))\n",
    "        max_card = int(self.static_thr.get(\"max_categorical_cardinality\", MAX_CATEGORICAL_CARDINALITY))\n",
    "\n",
    "        # --- Numeric drift ---\n",
    "        num_cols_cfg = list(tbl_conf.get(\"numeric_drift_cols\", []))\n",
    "        # Allow per-column overrides embedded in columns config (role=='numeric_feature')\n",
    "        for c in (tbl_conf.get(\"columns\") or []):\n",
    "            if isinstance(c, dict) and c.get(\"role\") == \"numeric_feature\":\n",
    "                name = c.get(\"name\")\n",
    "                if name and name not in num_cols_cfg:\n",
    "                    num_cols_cfg.append(name)\n",
    "\n",
    "        for colname in num_cols_cfg:\n",
    "            resolved = resolve_col_name(cur_df, colname)\n",
    "            if not resolved:\n",
    "                logger.warning(f\"Numeric drift column '{colname}' not in {sys}.{tbl}.\")\n",
    "                continue\n",
    "\n",
    "            psi, edges, cur_counts, ref_counts = compute_numeric_psi_and_buckets(\n",
    "                resolved, cur_df, ref_df, bins=bins, sample_size=sample_size\n",
    "            )\n",
    "\n",
    "            # Wazirstrian (Wasserstein) only if we have both histograms\n",
    "            waz = None\n",
    "            genshin = None\n",
    "            if edges and cur_counts is not None and ref_counts is not None:\n",
    "                waz = _wasserstein_from_buckets(edges, cur_counts, ref_counts)\n",
    "\n",
    "                # Genshin Shannon ratio = H(cur) / max(EPS, H(ref))\n",
    "                def _entropy_from_counts(cnts: List[int]) -> float:\n",
    "                    s = float(sum(cnts))\n",
    "                    if s <= 0: return 0.0\n",
    "                    H = 0.0\n",
    "                    for v in cnts:\n",
    "                        p = float(v) / s\n",
    "                        if p > 0:\n",
    "                            H -= p * math.log(p + EPSILON)\n",
    "                    return H\n",
    "                Hc = _entropy_from_counts(cur_counts)\n",
    "                Hr = _entropy_from_counts(ref_counts)\n",
    "                genshin = (Hc / max(EPSILON, Hr)) if Hr > 0 else (float('inf') if Hc > 0 else 1.0)\n",
    "\n",
    "            # Emit PSI (informational unless you treat as hard threshold)\n",
    "            if psi is not None:\n",
    "                status = \"PASS\" if psi <= psi_max else \"FAIL\"\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"numeric_drift_psi\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": colname,\n",
    "                    \"metric_value_num\": float(psi),\n",
    "                    \"metric_value\": f\"{psi:.4f}\",\n",
    "                    \"threshold\": f\"<= {psi_max:.2f}\",\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    \"reference_value\": ref_date,\n",
    "                }, partition_date=partition_date)\n",
    "\n",
    "            # Emit Wazirstrian (Wasserstein)\n",
    "            if waz is not None:\n",
    "                status = \"PASS\" if waz <= waz_max else \"FAIL\"\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"numeric_drift_wazirstrian\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": colname,\n",
    "                    \"metric_value_num\": float(waz),\n",
    "                    \"metric_value\": f\"{waz:.6f}\",\n",
    "                    \"threshold\": f\"<= {waz_max:.2f}\",\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    \"reference_value\": ref_date,\n",
    "                }, partition_date=partition_date)\n",
    "\n",
    "            # Emit Genshin Shannon ratio\n",
    "            if genshin is not None and math.isfinite(genshin):\n",
    "                status = \"PASS\" if genshin <= genshin_max else \"FAIL\"\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"numeric_drift_genshin_shannon\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": colname,\n",
    "                    \"metric_value_num\": float(genshin),\n",
    "                    \"metric_value\": f\"{genshin:.4f}\",\n",
    "                    \"threshold\": f\"<= {genshin_max:.2f}\",\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    \"reference_value\": ref_date,\n",
    "                }, partition_date=partition_date)\n",
    "            elif genshin is None and ref_df is None:\n",
    "                # No reference -> explicitly mark uncomputable for ratio-based metric\n",
    "                self._mark_uncomputable(\"numeric_drift_genshin_shannon\", sys, tbl, colname, partition_date=partition_date)\n",
    "\n",
    "        # --- Categorical drift ---\n",
    "        # If you have an explicit cfg, use it; otherwise infer from 'columns' that are not numeric_feature\n",
    "        cat_cols_cfg = list(tbl_conf.get(\"categorical_drift_cols\", []))\n",
    "        if not cat_cols_cfg:\n",
    "            for c in (tbl_conf.get(\"columns\") or []):\n",
    "                if isinstance(c, str):\n",
    "                    # plain column names — treat as candidates if not already in numeric list\n",
    "                    if c not in num_cols_cfg and c not in cat_cols_cfg:\n",
    "                        cat_cols_cfg.append(c)\n",
    "                elif isinstance(c, dict):\n",
    "                    nm = c.get(\"name\")\n",
    "                    role = c.get(\"role\")\n",
    "                    if nm and role != \"numeric_feature\" and nm not in num_cols_cfg and nm not in cat_cols_cfg:\n",
    "                        cat_cols_cfg.append(nm)\n",
    "\n",
    "        for colname in cat_cols_cfg:\n",
    "            resolved = resolve_col_name(cur_df, colname)\n",
    "            if not resolved:\n",
    "                continue\n",
    "\n",
    "            csi, cur_map, ref_map = compute_categorical_csi_and_buckets(\n",
    "                resolved, cur_df, ref_df, sample_size=sample_size, max_cardinality=max_card\n",
    "            )\n",
    "\n",
    "            # CSI (Chi-square-like)\n",
    "            if csi is not None:\n",
    "                status = \"PASS\" if csi <= csi_max else \"FAIL\"\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"categorical_drift_csi\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": colname,\n",
    "                    \"metric_value_num\": float(csi),\n",
    "                    \"metric_value\": f\"{csi:.2f}\",\n",
    "                    \"threshold\": f\"<= {csi_max:.2f}\",\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    \"reference_value\": ref_date,\n",
    "                }, partition_date=partition_date)\n",
    "\n",
    "            # Genshin Shannon ratio on categorical pmfs\n",
    "            genshin = None\n",
    "            if cur_map and ref_map:\n",
    "                def _entropy_from_map(m: Dict[str, int]) -> float:\n",
    "                    s = float(sum(m.values()))\n",
    "                    if s <= 0: return 0.0\n",
    "                    H = 0.0\n",
    "                    for v in m.values():\n",
    "                        p = float(v) / s\n",
    "                        if p > 0:\n",
    "                            H -= p * math.log(p + EPSILON)\n",
    "                    return H\n",
    "                Hc = _entropy_from_map(cur_map)\n",
    "                Hr = _entropy_from_map(ref_map)\n",
    "                genshin = (Hc / max(EPSILON, Hr)) if Hr > 0 else (float('inf') if Hc > 0 else 1.0)\n",
    "\n",
    "            if genshin is not None and math.isfinite(genshin):\n",
    "                status = \"PASS\" if genshin <= genshin_max else \"FAIL\"\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"categorical_drift_genshin_shannon\",\n",
    "                    \"source_system\": sys,\n",
    "                    \"table_name\": tbl,\n",
    "                    \"column_name\": colname,\n",
    "                    \"metric_value_num\": float(genshin),\n",
    "                    \"metric_value\": f\"{genshin:.4f}\",\n",
    "                    \"threshold\": f\"<= {genshin_max:.2f}\",\n",
    "                    \"status\": status,\n",
    "                    \"alert_flag\": \"Fail\" if status == \"FAIL\" else None,\n",
    "                    \"reference_value\": ref_date,\n",
    "                }, partition_date=partition_date)\n",
    "            elif genshin is None and ref_df is None:\n",
    "                self._mark_uncomputable(\"categorical_drift_genshin_shannon\", sys, tbl, colname, partition_date=partition_date)\n",
    "\n",
    "\n",
    "    def finalize_and_write(self):\n",
    "        \"\"\"\n",
    "        Sanitize -> normalize -> write the three canonical outputs:\n",
    "          1) DQ_METRIC_HISTORY (full audit)\n",
    "          2) DQ_MONITORING_REPORT (FAIL / alert_flag=Fail)\n",
    "          3) DQ_INTERNAL_ERRORS (explicit error metric types + UNCOMPUTABLE/ERROR)\n",
    "        Also performs cache cleanup and clears in-memory buffers.\n",
    "        \"\"\"\n",
    "        logger.info(\"Sanitizing metrics and preparing outputs (finalize_and_write).\")\n",
    "\n",
    "        try:\n",
    "            # 1) Sanitize collected metrics\n",
    "            try:\n",
    "                sanitized_metrics = sanitize_all_metrics_return_new(self.all_metrics or [])\n",
    "            except Exception:\n",
    "                logger.exception(\"sanitize_all_metrics_return_new failed; falling back to raw metrics.\")\n",
    "                sanitized_metrics = list(self.all_metrics or [])\n",
    "\n",
    "            # 2) Normalize sanitized metrics\n",
    "            try:\n",
    "                normalized_records = [self._normalize_metric_record(rec) for rec in sanitized_metrics]\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to normalize sanitized metrics; using sanitized output as-is.\")\n",
    "                normalized_records = sanitized_metrics\n",
    "\n",
    "            # 3) Build Spark DataFrame for history (canonical audit)\n",
    "            metrics_history_df = rows_to_df(self.spark, normalized_records, schema=metrics_history_schema)\n",
    "            if metrics_history_df is None or df_is_empty(metrics_history_df):\n",
    "                logger.warning(\"metrics_history_df is None/empty after rows_to_df. Aborting writes.\")\n",
    "                return\n",
    "\n",
    "            # 4) Write full metrics history (audit)\n",
    "            try:\n",
    "                history_ds = OUTPUT_DATASETS.get(\"metrics_history_df\", \"DQ_METRIC_HISTORY\")\n",
    "                write_dataset(history_ds, metrics_history_df, mode=\"append\")  # accepts legacy append=\n",
    "                logger.info(f\"Wrote full metrics history to '{history_ds}'. Rows: {len(normalized_records)}\")\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to write metrics history dataset.\")\n",
    "\n",
    "            # 5) Monitoring report: failed/alerted metrics\n",
    "            try:\n",
    "                monitoring_report_df = metrics_history_df.filter(\n",
    "                    (F.col(\"status\") == \"FAIL\") | (F.col(\"alert_flag\") == \"Fail\")\n",
    "                )\n",
    "            except Exception:\n",
    "                logger.exception(\"Error filtering metrics for monitoring report.\")\n",
    "                monitoring_report_df = None\n",
    "\n",
    "            try:\n",
    "                if monitoring_report_df is not None and not df_is_empty(monitoring_report_df):\n",
    "                    mon_ds = OUTPUT_DATASETS.get(\"monitoring_report_df\", \"DQ_MONITORING_REPORT\")\n",
    "                    write_dataset(mon_ds, monitoring_report_df, mode=\"append\")\n",
    "                    logger.info(f\"Monitoring report written to '{mon_ds}'.\")\n",
    "                else:\n",
    "                    logger.info(\"No failed/alerted metrics to write to monitoring report.\")\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to write monitoring report dataset.\")\n",
    "\n",
    "            # 6) Internal errors dataset: explicit error types or problematic status\n",
    "            try:\n",
    "                error_types = [\n",
    "                    \"metric_sanitization_failure\",\n",
    "                    \"data_load_error\",\n",
    "                    \"dq_internal_error\",\n",
    "                    \"dq_distribution_internal_error\",\n",
    "                ]\n",
    "                internal_errors_df = metrics_history_df.filter(\n",
    "                    F.col(\"metric_type\").isin(error_types)\n",
    "                    | (F.col(\"status\") == \"UNCOMPUTABLE\")\n",
    "                    | (F.col(\"status\") == \"ERROR\")\n",
    "                )\n",
    "            except Exception:\n",
    "                logger.exception(\"Error filtering metrics for internal errors dataset.\")\n",
    "                internal_errors_df = None\n",
    "\n",
    "            try:\n",
    "                if internal_errors_df is not None and not df_is_empty(internal_errors_df):\n",
    "                    err_ds = OUTPUT_DATASETS.get(\"internal_errors\", \"DQ_INTERNAL_ERRORS\")\n",
    "                    write_dataset(err_ds, internal_errors_df, mode=\"append\")\n",
    "                    logger.info(f\"Internal errors dataset written to '{err_ds}'.\")\n",
    "                else:\n",
    "                    logger.info(\"No internal errors to write.\")\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to write internal errors dataset.\")\n",
    "\n",
    "            # 7) Cleanup caches\n",
    "            logger.debug(\"Unpersisting cached DataFrames and clearing caches.\")\n",
    "            try:\n",
    "                for key in list(getattr(self, \"df_cache\", {}).keys()):\n",
    "                    df = self.df_cache.get(key)\n",
    "                    if df is not None:\n",
    "                        try:\n",
    "                            df.unpersist()\n",
    "                            logger.debug(f\"Unpersisted DataFrame for cache key: {key}\")\n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error unpersisting DataFrame for key {key}: {e}\")\n",
    "                    try:\n",
    "                        del self.df_cache[key]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    if key in getattr(self, \"df_count_cache\", {}):\n",
    "                        try:\n",
    "                            del self.df_count_cache[key]\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            except Exception:\n",
    "                logger.exception(\"Error during df_cache cleanup.\")\n",
    "\n",
    "            if getattr(self, \"metrics_history_df_cache\", None) is not None:\n",
    "                try:\n",
    "                    self.metrics_history_df_cache.unpersist()\n",
    "                    logger.debug(\"Unpersisted metrics history DataFrame cache.\")\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error unpersisting metrics history DataFrame: {e}\")\n",
    "                self.metrics_history_df_cache = None\n",
    "\n",
    "            # Finally clear in-memory metrics buffer\n",
    "            self.all_metrics = []\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"finalize_and_write encountered an unexpected exception: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_all(self):\n",
    "        \"\"\"Orchestrates all data quality checks for the current run date.\"\"\"\n",
    "        logger.info(f\"Starting DQ Monitor run for date: {self.curr_date_str}\")\n",
    "\n",
    "        # cache for this run (current partition only)\n",
    "        table_run_cache: Dict[Tuple[str, str], Optional[DataFrame]] = {}\n",
    "\n",
    "        # --- Phase 1: Load current/effective partition data ---\n",
    "        logger.info(\"Phase 1: Loading effective partitions for all configured tables.\")\n",
    "        for sys_name, sys_conf in (self.table_config or {}).items():\n",
    "            for tbl_name, tbl_conf in (sys_conf.get(\"tables\") or {}).items():\n",
    "                try:\n",
    "                    logger.debug(f\"Loading effective partition for {sys_name}.{tbl_name}@{self.curr_date_str}.\")\n",
    "                    cur_df = self._load_and_cache_df(sys_name, tbl_name, self.curr_date_str, tbl_conf=tbl_conf)\n",
    "                    table_run_cache[(sys_name, tbl_name)] = cur_df\n",
    "                except Exception:\n",
    "                    logger.exception(\"Load failed for %s.%s@%s\", sys_name, tbl_name, self.curr_date_str)\n",
    "                    table_run_cache[(sys_name, tbl_name)] = None\n",
    "                    self._emit_metric({\n",
    "                        \"metric_type\": \"data_load_error\",\n",
    "                        \"source_system\": sys_name,\n",
    "                        \"table_name\": tbl_name,\n",
    "                        \"metric_value\": f\"Load failure for {sys_name}.{tbl_name}@{self.curr_date_str}\",\n",
    "                        \"status\": \"ERROR\",\n",
    "                        \"alert_flag\": \"Fail\"\n",
    "                    }, partition_date=self.curr_date_str)\n",
    "\n",
    "        # --- Phase 2: Run table-level checks ---\n",
    "        logger.info(\"Phase 2: Running data quality checks.\")\n",
    "        for sys_name, sys_conf in (self.table_config or {}).items():\n",
    "            for tbl_name, tbl_conf in (sys_conf.get(\"tables\") or {}).items():\n",
    "                part = self.curr_date_str\n",
    "                logger.info(f\"Running checks for {sys_name}.{tbl_name}@{part}\")\n",
    "                cur_df = table_run_cache.get((sys_name, tbl_name))\n",
    "\n",
    "                try:\n",
    "                    self.check_row_count(sys_name, tbl_name, tbl_conf, cur_df, part)\n",
    "                    self.check_completeness(sys_name, tbl_name, tbl_conf, cur_df, part)\n",
    "                    self.check_uniqueness(sys_name, tbl_name, tbl_conf, cur_df, part)\n",
    "                    self.check_distribution_drift(sys_name, tbl_name, tbl_conf, cur_df, part)\n",
    "\n",
    "                    # If you really have these implemented, keep them. Otherwise, guard or remove.\n",
    "                    if hasattr(self, \"check_range\"):\n",
    "                        self.check_range(sys_name, tbl_name, tbl_conf, cur_df, part)\n",
    "                    if hasattr(self, \"check_date_logic\"):\n",
    "                        self.check_date_logic(sys_name, tbl_name, tbl_conf, cur_df, part)\n",
    "\n",
    "                    # Do NOT run cross-system inside this per-table loop. It runs in Phase 3.\n",
    "                    # Remove the typo/alias call that used to crash:\n",
    "                    # self.check_cross_system_join_consistency(...)\n",
    "                    # self.check_jocheck_cross_system_key_consistencyin_consistency(...)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Checks failed for %s.%s@%s\", sys_name, tbl_name, part)\n",
    "                    self._emit_metric({\n",
    "                        \"metric_type\": \"dq_internal_error\",\n",
    "                        \"source_system\": sys_name,\n",
    "                        \"table_name\": tbl_name,\n",
    "                        \"metric_value\": f\"Unhandled exception during checks: {e}\",\n",
    "                        \"status\": \"UNCOMPUTABLE\",\n",
    "                        \"alert_flag\": \"Fail\"\n",
    "                    }, partition_date=part)\n",
    "\n",
    "        # --- Phase 3: Cross-system consistency checks ---\n",
    "        logger.info(\"Phase 3: Running cross-system consistency checks.\")\n",
    "        for mapping in (self.cross_conf or []):\n",
    "            try:\n",
    "                self.check_cross_system_key_consistency(mapping, table_run_cache, self.curr_date_str)\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Cross-system check failed for mapping: %s\", mapping)\n",
    "                sf = mapping.get(\"source_system_from\", \"unknown\")\n",
    "                st = mapping.get(\"table_from\", \"unknown\")\n",
    "                sk = mapping.get(\"key_from\", \"unknown\")\n",
    "                self._emit_metric({\n",
    "                    \"metric_type\": \"cross_system_consistency_internal_error\",\n",
    "                    \"source_system\": sf,\n",
    "                    \"table_name\": st,\n",
    "                    \"column_name\": sk,\n",
    "                    \"metric_value\": f\"Unhandled exception during cross-system check: {e}\",\n",
    "                    \"status\": \"UNCOMPUTABLE\",\n",
    "                    \"alert_flag\": \"Fail\"\n",
    "                }, partition_date=self.curr_date_str)\n",
    "\n",
    "        # --- Phase 4: Finalize and Write Results ---\n",
    "        logger.info(\"Phase 4: Finalizing and writing results.\")\n",
    "        self.finalize_and_write()\n",
    "        logger.info(\"DQ Monitor run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Main Execution Block\n",
    "# -----------------------\n",
    "def main():\n",
    "    \"\"\"Entry point for the DQ monitoring script.\"\"\"\n",
    "    logger.info(\"Starting DQ Monitor script execution.\")\n",
    "\n",
    "    # Fetch table configurations dynamically or from a fixed source\n",
    "    # Ensure get_table_config returns a valid structure.\n",
    "    table_config = get_table_config(REGION)\n",
    "\n",
    "    if not table_config:\n",
    "        logger.error(\"Failed to load table configuration. Exiting.\")\n",
    "        # Optionally raise an exception or exit with a non-zero code\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Initialize the DQMonitor with all necessary configurations and Spark session\n",
    "        monitor = DQMonitor(\n",
    "            spark_session=spark,\n",
    "            table_config=table_config,\n",
    "            cross_conf=cross_system_key_harmonization,\n",
    "            curr_date_obj=CURR_DATE,\n",
    "            static_thr=dq_static_thresholds\n",
    "        )\n",
    "\n",
    "        # Execute all defined checks\n",
    "        monitor.run_all()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An unhandled exception occurred during DQMonitor execution.\")\n",
    "        # Attempt to write a final error metric if possible\n",
    "        try:\n",
    "            # Create a list of error metrics to be potentially written\n",
    "            error_metrics = [{\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": RUN_TS,\n",
    "                \"partition_date\": CURR_DATE_STR,\n",
    "                \"metric_type\": \"dq_script_failure\",\n",
    "                \"source_system\": \"N/A\",\n",
    "                \"table_name\": \"N/A\",\n",
    "                \"column_name\": \"N/A\",\n",
    "                \"metric_value\": f\"Script failed with unhandled exception: {e}\",\n",
    "                \"status\": \"UNCOMPUTABLE\",\n",
    "                \"alert_flag\": \"Fail\",\n",
    "                \"country\": REGION\n",
    "            }]\n",
    "            # Try to write these errors to the internal errors dataset\n",
    "            error_df = rows_to_df(spark, error_metrics, schema=metrics_history_schema)\n",
    "            write_dataset(OUTPUT_DATASETS[\"internal_errors\"], error_df, mode=\"append\")\n",
    "        except Exception as final_err:\n",
    "            logger.exception(\"Failed to write final error metric to internal errors dataset.\")\n",
    "\n",
    "    finally:\n",
    "        # Ensure Spark session is stopped if it was explicitly started by start_spark_session\n",
    "        # Or just rely on Dataiku's management if running within its environment.\n",
    "        # For standalone scripts, explicit stop might be needed.\n",
    "        logger.info(\"DQ Monitor script finished.\")\n",
    "        # spark.stop() # Uncomment if running in an environment where explicit stop is necessary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_ROW_COUNT_CHECK",
  "createdOn": 1757662777133,
  "creationTag": {
   "extendedProperties": {},
   "lastModifiedBy": {
    "login": "2001810"
   },
   "lastModifiedOn": 1757662777133,
   "versionNumber": 0
  },
  "creator": "2001810",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "kernelspec": {
   "display_name": "Python (env py39-scmac-mm-2024-05)",
   "language": "python",
   "name": "py-dku-venv-py39-scmac-mm-2024-05"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
